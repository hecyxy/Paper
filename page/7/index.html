<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="dreamer">
<meta property="og:type" content="website">
<meta property="og:title" content="万水千山">
<meta property="og:url" content="http://hcyxy.tech/page/7/index.html">
<meta property="og:site_name" content="万水千山">
<meta property="og:description" content="dreamer">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="万水千山">
<meta name="twitter:description" content="dreamer">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://hcyxy.tech/page/7/"/>





  <title>万水千山</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">万水千山</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">mail:hcy_xy@qq.com</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/10/04/基于互信息 左右熵挖掘新词/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/04/基于互信息 左右熵挖掘新词/" itemprop="url">基于互信息 左右熵挖掘新词</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-04T13:42:30+08:00">
                2018-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="成词标准一：熵-左右熵"><a href="#成词标准一：熵-左右熵" class="headerlink" title="成词标准一：熵-左右熵"></a>成词标准一：熵-左右熵</h3><p>通常我们认为两个片段可以成词的一个条件就是这个词语会在很多的语境中被提到。熵就是一个用来衡量这个维度的指标。熵是一种表示信息量的指标，熵越高就意味着信息含量越大，不确定性越高，越难以预测。通常对于一个随机变量 X, 它的熵可以被表示成：</p>
<p><img src="https://raw.githubusercontent.com/hecyxy/expand/master/src/main/resources/pic/v2-ab2f5003c9822fab020bdeddbb9c801f_hd.jpg" alt="pic" title="Markdown"></p>
<h3 id="成词标准二：内部聚合程度-互信息-mutual-information-amp-点间互信息-pointwise-mutual-information"><a href="#成词标准二：内部聚合程度-互信息-mutual-information-amp-点间互信息-pointwise-mutual-information" class="headerlink" title="成词标准二：内部聚合程度 - 互信息 (mutual information) &amp; 点间互信息 (pointwise mutual information)"></a>成词标准二：内部聚合程度 - 互信息 (mutual information) &amp; 点间互信息 (pointwise mutual information)</h3><p>互信息 (MI) 表示了两个随机变量 X, Y 共享的信息量。也可以说，互信息代表着知道了任意一个变量之后对另一个变量不确定性的减少。</p>
<p><img src="https://raw.githubusercontent.com/hecyxy/expand/master/src/main/resources/pic/v2-61bc0fa7cf105d74adb2e9b8cac342f9_hd.jpg" alt="pic2" title="Markdown"><br>步骤：</p>
<ol>
<li>对输入文本进行清洗和分词，没有任何分词词库情况下，直接将文本按照字符分割；<br>将字符两两组合作为候选词，因为需要前缀和后缀计算信息熵，需要存储长度为3的片段。后续设计到前后缀的查找饿词频的统计，选择trie树来存储数据。用3-gram序列构建前缀trie树和后缀树，trie树以单个字符为节点，每个节点记录从根节点到当前节点构成词汇出现的频次；</li>
<li>查询trie树，获取前缀和后缀的频次列表，计算候选词的左右信息熵以及候选词构成片段的左右信息熵。涉及到信息熵比较多，对每个信息熵作如下区分表及，（candidate为后选词，left为左边构成的片段，right为右边构成的片段）<br><img src="https://raw.githubusercontent.com/hecyxy/expand/master/src/main/resources/pic/v2-5a79911d6fc250cfc82960e2a936e992_hd.jpg" alt="avatar" title="Markdown"></li>
<li>查询trie树，获取候选词的词频以及左右片段的词频，有了词频可以计算实际出现的概率P(x,y）和期望出现的概率P(x)P(y)，从而计算出凝合度和互信息。为了防止冷启动前出现过高的概率p，预先跑一些基础词汇词频，在词频库基础上，保证刚开始拿到比较争取的概率p<br>增加准确率，设定词频和互信息阈值，排除低词频和低凝合度的词汇。所有满足条件的词汇，通过调用jieba的suggest_freq函数都能使其被分出来。词频大于2，PMI不小于24.<br>针对新词成词特点，分数计算：<br><img src="https://raw.githubusercontent.com/hecyxy/expand/master/src/main/resources/pic/v2-daf8dad77b7e7cc11f8c973d09f770cd_hd.jpg" alt="avatar" title="Markdown"></li>
</ol>
<p>分数由三个对应部分组成：<br><br>1）点间互信息：点间互信息越高，内部聚合程度越高<br>2）两个单词片段信息熵 h_r_l 和 h_l_r 的最小值：这个数值越大，则意味着两个单词一起出现的可能性越小<br>3）单词左右信息熵的最小值：这个数值越大就表示着候选词出现的语境越多，越有可能成词<br>因此，分数越高表示成词的可能性越大。</p>
<p>A、对于单词左右信息熵 ( h_l, h_r ) 为 0 的情况，迭代一轮，确认是否可能与左右的片段组成新词。 比如 “淘宝客” 这个词，先被分成了 “淘”、“宝”、“客”，在检测 “淘宝” 的时候，会发现它的右信息熵为 0，因此 “淘宝” 在当前上下文可能是另一个词的片段，所以通过下一轮迭代，检测 “淘宝” 和 “客” 能否成词。<br><br>B、最后根据词频和score的乘积排序，筛选出 top 5 的词汇作为新词。淘宝客这个例子中筛选出来的 top 5 新词结果如下：<br><img src="https://raw.githubusercontent.com/hecyxy/expand/master/src/main/resources/pic/v2-6fc041e4943846b55cd6183e82239f5d_hd.jpg" alt="avatar" title="Markdown"></p>
<p>实际应用中，词库实际上是不断被完善的，因此词库越完善，后续满足条件的未登录词会越少。我们对新词的挖掘也是基于内部的词库。试着跑一段时间内被反作弊系统悟空删除的内容，最后出来的前几个基本上都是站内被提及比较多的营销关键词了</p>
<h4 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h4><p>I(X,Y) = ∫x∫yP(X,Y)log(P(X,Y)/P(x) * P(y))= H(Y)-H(Y|X)<br>H(Y) = -∫yP(Y)logP(Y)<br>H(Y|X)则表示在已知X的情况下，Y的不确定度,可以看出，I(X,Y)可以解释为由X引入而使Y的不确定度减小的量，这个减小的量为H(Y|X)所以，如果X,Y关系越密切，I(X,Y)就越大.</p>
<p>在做文本分类时，我们可能会判断一个词和某类的相关程度，但是计算时，未考虑词频的影响<br>（可以让互信息的结果再乘上tf-idf因子，从而将特征频率与特征分布考虑进去）</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/09/08/结巴分词/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/08/结巴分词/" itemprop="url">结巴分词</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-08T10:24:29+08:00">
                2018-09-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>1、导入词库<br>load main dict  -&gt;  （词，词频，词性）<br>load model -&gt; 状态转移矩阵  发射矩阵  初始概率矩阵<br>2、sentenceProcess<br>  ● createDAG 根绝句子创建有向无环图 比如[0:[0,1,2],1:[1],2:[2,3,4]]   0,0-1,0-2都是可以成词的<br>  ● calc(sentence,DAG) 返回route计算概率最大的词语 比如2:[2,3,4] 最大概率是2-4<br>接着针对Map&lt;Integer, Pair<integer>&gt;中的最大概率词语<br>如果词典中包含该词语，直接将词语加入List<string> tokens<br>  ● 如果没有该词，利用维特比算法【基于动态规划寻找最大概率路径】<br>和树结构中寻找最短路径不同的是，这里的观察值是有序列的，每个序列有n个状态，长度为T，时间复杂度TN^2<br>关键代码如下：<br>public static int[] compute(int[] obs, int[] states, double[] start_p, double[][] trans_p, double[][] emit_p)<br>{<br>    double[][] V = new double[obs.length][states.length];<br>    int[][] path = new int[states.length][obs.length];<br>    for (int y : states)<br>    {<br>        V[0][y] = start_p[y] <em> emit_p[y][obs[0]];<br>        path[y][0] = y;<br>    }<br>    for (int t = 1; t &lt; obs.length; ++t)<br>    {<br>        int[][] newpath = new int[states.length][obs.length];<br>        for (int y : states)<br>        {<br>            double prob = -1;<br>            int state;<br>            for (int y0 : states)<br>            {<br>                double nprob = V[t - 1][y0] </em> trans_p[y0][y] * emit_p[y][obs[t]];<br>                if (nprob &gt; prob)<br>                {<br>                    prob = nprob;<br>                    state = y0;<br>                    // 记录最大概率<br>                    V[t][y] = prob;<br>                    // 记录路径<br>                    System.arraycopy(path[state], 0, newpath[y], 0, t);<br>                    newpath[y][t] = y;<br>                }<br>            }<br>        }<br>        path = newpath;<br>    }<br>    double prob = -1;<br>    int state = 0;<br>    for (int y : states)<br>    {<br>        if (V[obs.length - 1][y] &gt; prob)<br>        {<br>            prob = V[obs.length - 1][y];<br>            state = y;<br>        }<br>    }<br>    return path[state];<br>}</string></integer></p>
<h2 id="基于动态规划，每步寻找概率最大的状态，使观察值最大。"><a href="#基于动态规划，每步寻找概率最大的状态，使观察值最大。" class="headerlink" title="基于动态规划，每步寻找概率最大的状态，使观察值最大。"></a>基于动态规划，每步寻找概率最大的状态，使观察值最大。</h2><p>1、HMM隐马尔科夫模型<br>五元组：<br>  ● StatusSet 状态值集合<br>  ● ObservedSet 观察值集合<br>  ● TransProbMatrix 转移概率矩阵<br>  ● EmitProbMatrix 发射概率矩阵<br>  ● InitStatus 初始状态分布<br>2、主要用于解决三类问题<br>a)参数(StatusSet,TransProbMatrix,EmitRobMatrix,InitStatus)已知的情况下，求解观察值序列。(Forward-backward算法)<br>b)参数(ObservedSet,TransProbMatrix,EmitRobMatrix,InitStatus)已知的情况下，求解状态值序列。(viterbi算法)<br>c)参数(ObservedSet)已知的情况下，求解(TransProbMatrix,EmitRobMatrix,InitStatus)。(Baum-Welch算法)<br>第二种问题最长用：【中文分词】【语音识别】【新词发现】【词性标注】</p>
<p>StatusSet &amp; ObservedSet<br>状态值集合(B,M,E,S)代表每个词再词语中的位置,S代表单字成词<br>观察值集合就是所有汉字（一个句子）<br>状态值就是我们要求的值，HMM模型进行中文分词，输入一个句子（观察值序列），输出每个字的状态值，如<br>小明硕士毕业于中国科学院计算所<br>输出的状态序列为：<br>BEBEBMEBEBMEBES<br>再根据状态序列进行切词：<br>BE/BE/BME/BE/BME/BE/S<br>切词结果为：<br>小明/硕士/毕业于/中国/科学院/计算/所<br>同时我们可以注意到：<br>B后面只可能接(M or E)，不可能接(B or S)。而M后面也只可能接(M or E)，不可能接(B, S)<br>上面介绍了五元组中的两元【StatusSet、ObservedSet】，还有剩下三元，通过Viterbi算法串接起来，ObservedSet序列值是Viterbi的输入，而StatusSet序列值是Viterbi的输出，输入和输出之间Viterbi还需要借助三个模型参数<br>InitStatus初始概率分布：</p>
<p>#B<br>-0.26268660809250016</p>
<p>#E<br>-3.14e+100</p>
<p>#M<br>-3.14e+100</p>
<p>#S<br>-1.4652633398537678<br>即句子第一个字属于{B,E,M,S}这四种状态的概率，E、M概率都是0，即开头第一个字只能是B或S<br>TransProbMatrix转移概率矩阵<br>即BEMS x BEMS【数值是概率求对数后的值】<br>-3.14e+100 -0.510825623765990 -0.916290731874155 -3.14e+100<br>-0.5897149736854513 -3.14e+100 -3.14e+100 -0.8085250474669937<br>-3.14e+100 -0.33344856811948514 -1.2603623820268226 -3.14e+100<br>-0.7211965654669841 -3.14e+100 -3.14e+100 -0.6658631448798212<br>比如TransProbMatrix[0][0]表示从状态B转移到状态B的概率为0<br>EmitProbMatrix<br>发射概率也是一个条件概率，根据HMM三个基本假设其中的观察值独立性假设，观察值只取决于当前状态值，即<br>P(Observed[i], Status[j]) = P(Status[j]) * P(Observed[i] | Status[j])<br>其中P(Observed[i]|Status[j])这个值就是从EmitProbMatrix中获取。<br>EmitProbMatrix示例如下</p>
<p>#B<br>耀:-10.460283,涉:-8.766406,谈:-8.039065,伊:-7.682602,洞:-8.668696,…</p>
<p>#E<br>耀:-9.266706,涉:-9.096474,谈:-8.435707,伊:-10.223786,洞:-8.366213,…</p>
<p>#M<br>耀:-8.47651,涉:-10.560093,谈:-8.345223,伊:-8.021847,洞:-9.547990,….</p>
<p>#S<br>蘄:-10.005820,涉:-10.523076,唎:-15.269250,禑:-17.215160,洞:-8.369527…<br>虽然EmitProbMatrix也称为矩阵，这个矩阵太稀疏了，实际工程中一般是将上面四行发射转移概率存储为4个Map<br>HMM算法过程<br>输入：小明硕士毕业于中国科学院计算所<br>1）定义变量<br>二维数组 weight[4][15]，4是状态数(0:B,1:E,2:M,3:S)，15是输入句子的字数。比如 weight[0][2] 代表 状态B的条件下，出现’硕’这个字的可能性。<br>二维数组 path[4][15]，4是状态数(0:B,1:E,2:M,3:S)，15是输入句子的字数。比如 path[0][2] 代表 weight[0][2]取到最大时，前一个字的状态，比如 path[0][2] = 1, 则代表 weight[0][2]取到最大时，前一个字(也就是明)的状态是E。记录前一个字的状态是为了使用viterbi算法计算完整个 weight[4][15] 之后，能对输入句子从右向左地回溯回来，找出对应的状态序列<br>2）使用InitStatus对weight二维数组进行初始化<br>已知InitStatus如下：</p>
<p>#B<br>-0.26268660809250016</p>
<p>#E<br>-3.14e+100</p>
<p>#M<br>-3.14e+100</p>
<p>#S<br>-1.4652633398537678<br>且由EmitProbMatrix可得出：<br>Status(B) -&gt; Observed(小)  :  -5.79545<br>Status(E) -&gt; Observed(小)  :  -7.36797<br>Status(M) -&gt; Observed(小)  :  -5.09518<br>Status(S) -&gt; Observed(小)  :  -6.2475<br>3)初始化weight<br>weight[0][0] = -0.26268660809250016 + -5.79545 = -6.05814<br>weight[1][0] = -3.14e+100 + -7.36797 = -3.14e+100<br>weight[2][0] = -3.14e+100 + -5.09518 = -3.14e+100<br>weight[3][0] = -1.4652633398537678 + -6.2475 = -7.71276<br>3）确定边界条件和路径回溯<br>边界条件：对于每个句子，最后一个字的状态只可能是 E 或者 S，不可能是 M 或者 B。<br>所以在本文的例子中我们只需要比较 weight[1(E)][14] 和 weight[3(S)][14] 的大小即可<br>weight[1][14] = -102.492;<br>weight[3][14] = -101.632;<br>所以S&gt;E，对于路径回溯的起点是path[3][14]<br>回溯路径：SEBEMBEBEMBEBEB<br>倒序一下就是：BE/BE/BME/BE/BME/BE/S<br>切词结果：小明/硕士/毕业于/中国/科学院/计算/所<br>给定我们一个模型，我们对模型进行载入完毕之后，只要运行一遍Viterbi算法，就可以找出每个字对应的状态，根据状态也就可以对句子进行分</p>
<p>模型训练问题<br>以上的前提是基于模型来进行切词，假设手头上HMM模型已经是被训练好的，即InitStatus、TransProbMatrix、EmitProbMatrix三个模型关键参数都是已知的。这三个参数其实也是基于已分词完毕的语料进行统计计算，计算出相应的频率和条件概率就可以算出三个参数</p>
<p>HMM模型连个个基本假设：<br>  ● 齐次性假设（状态和当前时刻无关）<br>P(Status[i]|Status[i-1],Status[i-2],… Status[1]) = P(Status[i] | Status[i-1])<br>  ● 观察值独立性假设（观察值只取决于当前状态值）<br>P(Observed[i] | Status[i],Status[i-1],…,Status[1]) = P(Observed[i] | Status[i])</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/14/无锁编程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/14/无锁编程/" itemprop="url">无锁编程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-14T22:54:45+08:00">
                2018-07-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/分布式系统/" itemprop="url" rel="index">
                    <span itemprop="name">分布式系统</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>分布式系统中，多线程中，为了保持一致性，锁通常是一种很好用的方式，但是锁是涉及到操作系统层面的切换，是低效的一种方式，要尽量使用无锁的编程思想。</p><br><p>锁是一种悲观的策略，每次临界区操作都会产生冲突。无锁是一种乐观的策略，假设对资源的访问没有冲突。其有两大优点：</p>


<p>1.高并发情况下，比锁拥有更好的性能 <br><br>2.死锁免疫 <br><br>（一：基础类）<br>1.与众不同的并发策略：比较交换（CAS）</p>
<pre><code>与锁相比，使用比较交换（下文简称CAS）会使程序看起来更加复杂一些。但由于其非阻塞性，它对死锁问题天生免疫，并且，线程间的相互影响也远远比基于锁的方式要小。更为重要的是，使用无锁的方式完全没有锁竞争带来的系统开销，也没有线程间频繁调度带来的开销，因此，它要比基于锁的方式拥有更优越的性能。

CAS算法的过程是这样：它包含三个参数CAS(V,E,N)。V表示要更新的变量，E表示预期值，N表示新值。仅当V值等于E值时，才会将V的值设为N，如果V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，CAS返回当前V的真实值。CAS操作是抱着乐观的态度进行的，它总是认为自己可以成功完成操作。当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理，CAS操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。

简单地说，CAS需要你额外给出一个期望值，也就是你认为这个变量现在应该是什么样子的。如果变量不是你想象的那样，那说明它已经被别人修改过了。你就重新读取，再次尝试修改就好了。

在硬件层面，大部分的现代处理器都已经支持原子化的CAS指令。在JDK 5.0以后，虚拟机便可以使用这个指令来实现并发操作和并发数据结构，并且，这种操作在虚拟机中可以说是无处不在。
</code></pre><p>2.无锁的线程安全整数：AtomicInteger  LongAddr系列</p>
<pre><code>为了让Java程序员能够受益于CAS等CPU指令，JDK并发包中有一个atomic包，里面实现了一些直接使用CAS操作的线程安全的类型。其中，最常用的一个类，应该就是AtomicIn-teger。你可以把它看做是一个整数。但是与Inte-ger不同，它是可变的，并且是线程安全的。对其进行修改等任何操作，都是用CAS指令进行的。这里简单列举一下AtomicInteger的一些主要方法，对于其他原子类，操作也是非常类似的。
</code></pre><p>3.Java中的指针：Unsafe类</p>
<p> 在AtomicInteger中compareAndSet()方法：</p>
<pre><code>public final boolean compareAndSet(int expect, int update) {

    return unsafe.compareAndSwapInt(this, valueOffset, expect, update);

}
</code></pre><p>有一个特殊的变量unsafe，它是sun.misc.Unsafe类型。这个类封装了一些不安全的操作，类似C语言中指针的操作。</p>
<p>   public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);</p>
<pre><code>方法是一个navtive方法，它的参数含义是：

var1为给定的对象

var2为对象内的偏移量（其实就是一个字段到对象头部的偏移量，通过这个偏移量可以快速定位字段）

var4表示期望值

xvar5要设置的值。如果指定的字段的值等于var4，那么就会把它设置为var5。
</code></pre><p>不难看出，compareAndSwapInt()方法的内部，必然是使用CAS原子指令来完成的。<br>注意：根据Java类加载器的工作原理，应用程序的类由App Loader加载。而系统核心类，如rt.jar中的类由Bootstrap类加载器加载。Bootstrap加载器没有Java对象的对象，因此试图获得这个类加载器会返回null。所以，当一个类的类加载器为null时，说明它是由Bootstrap加载的，而这个类也极有可能是rt.jar中的类。就是由rt.jar加载<br>netty中的无锁队列底层MPSC队列（JCTools）无锁就是利用这个实现</p>
<p>4.无锁的对象引用：AtomicReference</p>
<pre><code>AtomicReference和AtomicInteger非常类似，不同之处就在于AtomicInteger是对整数的封装，而AtomicReference则对应普通的对象引用。也就是它可以保证你在修改对象引用时的线程安全性。在介绍AtomicReference的同时，我希望同时提出一个有关原子操作的逻辑上的不足。

之前我们说过，线程判断被修改对象是否可以正确写入的条件是对象的当前值和期望值是否一致。这个逻辑从一般意义上来说是正确的。但有可能出现一个小小的例外，就是当你获得对象当前数据后，在准备修改为新值前，对象的值被其他线程连续修改了两次，而经过这两次修改后，对象的值又恢复为旧值。这样，当前线程就无法正确判断这个对象究竟是否被修改过。
</code></pre><p>5.带有时间戳的对象引用：AtomicStampedReference</p>
<pre><code>AtomicReference无法解决上述问题的根本因为是对象在修改过程中，丢失了状态信息。

AtomicStampedReference，它内部不仅维护了对象值，还维护了一个时间戳（我这里把它称为时间戳，实际上它可以使任何一个整数来表示状态值）。当AtomicStampedReference对应的数值被修改时，除了更新数据本身外，还必须要更新时间戳。可以解决ABA问题

当AtomicStampedReference设置对象值时，对象值以及时间戳都必须满足期望值，写入才会成功。因此，即使对象值被反复读写，写回原值，只要时间戳发生变化，就能防止不恰当的写入。
</code></pre><p>6.数组也能无锁：AtomicIntegerArray</p>
<pre><code>除了提供基本数据类型外，JDK还为我们准备了数组等复合结构。当前可用的原子数组有：AtomicIntegerArray、AtomicLongArray和AtomicReferenceArray，分别表示整数数组、long型数组和普通的对象数组。
</code></pre><p>7.让普通变量也享受原子操作：AtomicIntegerFieldUpdater</p>
<pre><code>将普通变量也变成线性安全的。

在原子包里还有一个实用的工具类AtomicIn-tegerFieldUpdater。它可以让你在不改动（或者极少改动）原有代码的基础上，让普通的变量也享受CAS操作带来的线程安全性，这样你可以修改极少的代码，来获得线程安全的保证。

根据数据类型不同，这个Updater有三种，分别是AtomicIntegerFieldUpdater、AtomicLong-FieldUpdater和AtomicReferenceFieldUpdater。顾名思义，它们分别可以对int、long和普通对象进行CAS修改。

虽然AtomicIntegerFieldUpdater很好用，但是还是有几个注意事项：

第一，Updater只能修改它可见范围内的变量。因为Updater使用反射得到这个变量。如果变量不可见，就会出错。比如如果score申明为private，就是不可行的。

第二，为了确保变量被正确的读取，它必须是volatile类型的。如果我们原有代码中未申明这个类型，那么简单地申明一下就行，这不会引起什么问题。

第三，由于CAS操作会通过对象实例中的偏移量直接进行赋值，因此，它不支持static字段（Unsafe. objectFieldOffset()不支持静态变量）。    
</code></pre><p>8.SynchronousQueue的实现</p>
<pre><code>在对线程池的介绍中，提到了一个非常特殊的等待队列SynchronousQueue。Syn-chronousQueue的容量为0，任何一个对Syn-chronousQueue的写需要等待一个对Syn-chronousQueue的读,因此，Syn-chronousQueue与其说是一个队列，不如说是一个数据交换通道。

对SynchronousQueue来说，它将put()和take()两个功能截然不同的操作抽象为一个共通的方法Transferer.transfer()。
</code></pre><p>Object transfer(Object e, boolean timed, long nanos)</p>
<pre><code>当参数e为非空时，表示当前操作传递给一个消费者，如果为空，则表示当前操作需要请求一个数据。timed参数决定是否存在timeout时间，nanos决定了timeout的时长。如果返回值非空，则表示数据已经接受或者正常提供，如果为空，则表示失败（超时或者中断）。

SynchronousQueue内部会维护一个线程等待队列。等待队列中会保存等待线程以及相关数据的信息。比如，生产者将数据放入Syn-chronousQueue时，如果没有消费者接收，那么数据本身和线程对象都会打包在队列中等待（因为SynchronousQueue容积为0，没有数据可以正常放入）。
 1. 如果等待队列为空，或者队列中节点的类型和本次操作是一致的，那么将当前操作压入队列等待。比如，等待队列中是读线程等待，本次操作也是读，因此这两个读都需要等待。进入等待队列的线程可能会被挂起，它们会等待一个“匹配”操作。

 2. 如果等待队列中的元素和本次操作是互补的（比如等待操作是读，而本次操作是写），那么就插入一个“完成”状态的节点，并且让他“匹配”到一个等待节点上。接着弹出这两个节点，并且使得对应的两个线程继续执行。

 3. 如果线程发现等待队列的节点就是“完成”节点，那么帮助这个节点完成任务。其流程和步骤2是一致的。
</code></pre><p>（二：无锁数据结构）</p>
<p>1.RingBuffer<br>一个环状的数据结构，一个生产者，一个消费者，在linux任务调度里应用很多</p>
<p>2.disruptor数据结构<br>是一个生产者、消费者的设计模式，也是一个ringbuffer，但是适合多个生产者、消费者。底层优化到操作系统二级缓存，还有其他的一些优化，这里不作具体介绍；</p>
<p>3.JCTools工具包<br>原本jdk里面缺少了对无锁队列的支持，这是提供了各种无锁队列，比如单生产者单消费者、单生产者多消费者、多生产者单消费者、多生产者多消费者等。netty的新版中，也用MPSC代替了原本的LinkedBlockingQueue，经过测试，性能大大提升。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/08/动态加载Jar/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/动态加载Jar/" itemprop="url">动态加载Jar</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T10:24:29+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/JAVA/" itemprop="url" rel="index">
                    <span itemprop="name">JAVA</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>应用场景，程序中使用不同的jdbc包，来连接数据库。</p>
<p>动态加载jdbc驱动：<br>//URL u = new URL(“file:C:\working\eclipse\Testsb\lib\“+mysqlJdbcFile);<br>URL u = new URL(“jar:file:lib/“ + mysqlJdbcFile + “!/“);<br>        String classname = jdbcVersionMap.get(mysqlJdbcFile);<br>        URLClassLoader ucl = new URLClassLoader(new URL[] { u });<br>        Driver d = (Driver)Class.forName(classname, true, ucl).newInstance();<br>        DriverShim driver = new DriverShim(d);<br>        DriverManager.registerDriver(driver);<br>使用完之后卸载这个driver：<br>DriverManager.deregisterDriver()</p>
<p>为什么Driver要用DriverShim包装一下呢？<br>因为DriverManager拒绝使用不是由SystemClassLoader加载的驱动程序。<br>解决方法就是创建一个shim类实现Driver的接口。因为DriverShim是由系统加载器加载的，所以它可以实现注册；<br>因为Class.forName不一定使用的是系统类加载器，而是使用调用者的类加载器，比如使用了自定义类加载器；<br>public static Class&lt;?&gt; forName(String className)<br>           throws ClassNotFoundException {<br>return forName0(className, true, ClassLoader.getCallerClassLoader());<br>   }<br>通过 ClassLoader.getCallerClassLoader()获取类加载器：<br>// Returns the invoker’s class loader, or null if none.<br>   // NOTE: This must always be invoked when there is exactly one intervening<br>   // frame from the core libraries on the stack between this method’s<br>   // invocation and the desired invoker.<br>   static ClassLoader getCallerClassLoader() {<br>       // NOTE use of more generic Reflection.getCallerClass()<br>       Class caller = Reflection.getCallerClass(3);<br>       // This can be null if the VM is requesting it<br>       if (caller == null) {<br>           return null;<br>       }<br>       // Circumvent security check since this is package-private<br>       return caller.getClassLoader0();  </p>
<h1 id=""><a href="#" class="headerlink" title="   }  "></a>   }  </h1><p>扩展，不仅仅是加载jdbc驱动，更广泛一些加载一些其他的实现包<br>URL url = new URL(“file:D:/test.jar”);<br>URLClassLoader myclassLoader1 = new URLClassLoader(new URL[]                        {},Thread.currentThread.getContextClassLoader());<br>Class&lt;?&gt; myclass1 = myClassLoader1.loadClass(“com.antmq.dynamic.TestAction”);//后面加载的为其实现类<br>Abstraction action1 = (AbstactAction)myClass1.newInstance();//AbstractionAction是一个接口<br>String str1 =action1.action();</p>
<p>HDFS上的jar包，不能直接用，因为url不支持hdfs协议。<br>URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());<br>这样就支持hdfs协议了，path就可以转url了，当然，得加载下hdfs相关配置文件，获取FileSystem实例<br>Path path = fs.getFileStatus(new Path(jarPath)).getPath();<br>URL url = path.toUri().toURL();<br>classLoader.addJar(url);  </p>
<h1 id="如果加载的是mapreduce程序的jar包，只能是本地路径，不能使用hdfs路径，在setJarByClass方法中，是需要寻找到jar包绝对路径的"><a href="#如果加载的是mapreduce程序的jar包，只能是本地路径，不能使用hdfs路径，在setJarByClass方法中，是需要寻找到jar包绝对路径的" class="headerlink" title="如果加载的是mapreduce程序的jar包，只能是本地路径，不能使用hdfs路径，在setJarByClass方法中，是需要寻找到jar包绝对路径的"></a>如果加载的是mapreduce程序的jar包，只能是本地路径，不能使用hdfs路径，在setJarByClass方法中，是需要寻找到jar包绝对路径的</h1><p>使用一段时间后发现，进程是不停的，需要加载的jar包是可能需要修改的，所以导致多次加载不同版本的jar包之后，程序会有bug，读不到最新的jar包，不太可能更新jar包就重启下程序<br>于是，又研究了下classloarder，在当前线程加载jar包比较保险，线程结束会释放掉，保证每次加载都是最新的，而且也不占内存<br>(URLClassLoader)ClassLoader.getSystemClassLoader();    </p>
<p>改为当前线程类加载器<br>(URLClassLoader)Thread.currentThread().getContextClassLoader(); </p>
<p>====================================================================<br>java.lang.ClassLoader几个重要方法<br>//加载指定名称（包括包名）的二进制类型，供用户调用的接口<br>public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException{ … }  </p>
<p>//加载指定名称（包括包名）的二进制类型，同时指定是否解析（但是这里的resolve参数不一定真正能达到解析的效果），供继承用<br>protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException{ … }  </p>
<p>//findClass方法一般被loadClass方法调用去加载指定名称类，供继承用<br>protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { … }  </p>
<p>//定义类型，一般在findClass方法中读取到对应字节码后调用，可以看出不可继承<br>//（说明：JVM已经实现了对应的具体功能，解析对应的字节码，产生对应的内部数据结构放置到方法区，所以无需覆写，直接调用就可以了）<br>protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len) throws ClassFormatError{ … }  </p>
<p>自底向上判断是否装载，自顶向下尝试装载；<br>Class.forName(String name)默认会使用调用类的类加载器来进行类加载<br>在不指定父类加载器的情况下，默认采用系统类加载器</p>
<p>java程序动态扩展方式：<br>调用Class.forName<br>用户自定义加载器<br>1、首先检查请求的类型是否已经被这个类装载器装载到命名空间中了，如果已经装载，直接返回；否则转入步骤2；<br>　　2、委派类加载请求给父类加载器（更准确的说应该是双亲类加载器，真实虚拟机中各种类加载器最终会呈现树状结构），如果父类加载器能够完成，则返回父类加载器加载的Class实例；否则转入步骤3；<br>　　3、调用本类加载器的findClass（…）方法，试图获取对应的字节码，如果获取的到，则调用defineClass（…）导入类型到方法区；如果获取不到对应的字节码或者其他原因失败，返回异常给loadClass（…）， loadClass（…）转而抛异常，终止加载过程（注意：这里的异常种类不止一种）。<br>标准扩展类加载器和系统类加载器及其父类（java.net.URLClassLoader和java.security.SecureClassLoader）都没有覆写java.lang.ClassLoader中默认的加载委派规则—loadClass（…）方法。有关java.lang.ClassLoader中默认的加载委派规则前面已经分析过，如果父加载器为null，则会调用本地方法进行启动类加载尝试。</p>
<p>线程上下文类加载器 解决SPI（service provider interface）<br>SPI 的接口是 Java 核心库的一部分，是由引导类加载器来加载的；SPI 实现的 Java 类一般是由系统类加载器来加载的。引导类加载器是无法找到 SPI 的实现类的，因为它只加载 Java 的核心库。它也不能代理给系统类加载器，因为它是系统类加载器的祖先类加载器。也就是说，类加载器的代理模式无法解决这个问题</p>
<p>线程上下文类加载器正好解决了这个问题。如果不做任何的设置，Java 应用的线程的上下文类加载器默认就是系统上下文类加载器。在 SPI 接口的代码中使用线程上下文类加载器，就可以成功的加载到 SPI 实现的类。线程上下文类加载器在很多 SPI 的实现中都会用到。<br>使用线程上下文类加载器，可以在执行线程中抛弃双亲委派加载链模式，使用线程上下文里的类加载器加载类<br>Web容器、hot swap都使用了线程上下文加载器</p>
<h2 id="一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程"><a href="#一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程" class="headerlink" title="一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程"></a>一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程</h2><p>Tomcat 来说，每个 Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得 Web 应用自己的类的优先级高于 Web 容器提供的类。</p>
<p>OSGi是 Java 上的动态模块系统。它为开发人员提供了面向服务和基于组件的运行环境，并提供标准的方式用来管理软件的生命周期。OSGi 已经被实现和部署在很多产品上，在开源社区也得到了广泛的支持。Eclipse就是基于OSGi 技术来构建的<br>OSGi 中的每个模块都有对应的一个类加载器。它负责加载模块自己包含的 Java 包和类。当它需要加载 Java 核心库的类时（以 java开头的包和类），它会代理给父类加载器（通常是启动类加载器）来完成。当它需要加载所导入的 Java 类时，它会代理给导出此 Java 类的模块来完成加载。<br>　OSGi 模块的这种类加载器结构，使得一个类的不同版本可以共存在 Java 虚拟机中，带来了很大的灵活性。</p>
<ol>
<li>当高层提供了统一接口让低层去实现，同时又要是在高层加载（或实例化）低层的类时，必须通过线程上下文类加载器来帮助高层的ClassLoader找到并加载该类。 </li>
<li>当使用本类托管类加载，然而加载本类的ClassLoader未知时，为了隔离不同的调用者，可以取调用者各自的线程上下文类加载器代为托管。<br>简而言之就是ContextClassLoader默认存放了AppClassLoader的引用，由于它是在运行时被放在了线程中，所以不管当前程序处于何处（BootstrapClassLoader或是ExtClassLoader等），在任何需要的时候都可以用Thread.currentThread().getContextClassLoader()取出应用程序类加载器来完成需要的操作。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/08/Java线上问题排查/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/Java线上问题排查/" itemprop="url">Java线上问题排查</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T10:22:06+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/JAVA/" itemprop="url" rel="index">
                    <span itemprop="name">JAVA</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>线上问题调查常用命令<br>linux性能检测工具<br>     CPU<br>          基本概念:<br>               1.上下文切换<br>               2.运行队列<br>                    每个处理器运行队列应该不超过1-3个线程<br>               3.load<br>                    一段时间内cpu正在处理以及等待cpu处理的进程数之和的统计信息</p>
<pre><code>               安全load值:一般是cpu个数
                --------------------------------------------------------------
               uptime/w
               uptime命令可以显示系统运行多久、当前有多少的用户登录、在过去的1，5，15分钟里平均负载时多少
          4.CPU利用率
               User Time           65-70%(经验值:正常的均衡比例)
               System Time         30-35%(经验值:正常的均衡比例)
               Wait IO             
               Idel                0-5%(经验值:正常的均衡比例)
               --------------------------------------------------------------
               cpu信息:
                    cat /proc/cpuinfo
                    grep &apos;processor&apos; /proc/cpuinfo | wc -l
               --------------------------------------------------------------
               vmstat 1 100
                    procs（进程）：
                         r: 运行队列中进程数量
                         b: 等待IO的进程数量
                    memory（内存）：
                         swpd: 使用虚拟内存大小
                         free: 可用内存大小
                         buff: 用作缓冲的内存大小
                         cache: 用作缓存的内存大小
                    swap：
                         si: 每秒从交换区写到内存的大小
                         so: 每秒写入交换区的内存大小
                    io：（现在的Linux版本块的大小为1024bytes）
                         bi: 每秒读取的块数
                         bo: 每秒写入的块数
                    system：
                         in: 每秒中断数，包括时钟中断。
                         cs: 每秒上下文切换数。
                    cpu（以百分比表示）：
                         us: 用户进程执行时间(user time)
                         sy: 系统进程执行时间(system time)
                         id: 空闲时间(包括IO等待时间)
                         wa: 等待IO时间
                    例子:
                    procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
                     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
                     0  0      0 3780980  45064 1387444    0    0    55     3   41   67  1  0 99  1  0
               --------------------------------------------------------------
               top
                    H show all thread by process
                    P 按cpu占用排序
                    M 按内存占用排序
                    1 显示cpu个数
               --------------------------------------------------------------
               mpstat -P ALL 1 (Multiprocessor Statistics)
               --------------------------------------------------------------
               sar 能够查看历史数据, 也能查看实时数据
               sar -q 查看load情况
               sar -u 查看cpu使用率
               sar -q -f /var/log/sa/saXX 查看之前某一天的历史记录

          5.中断

Memory
     Virtual Memory
     kswapd
          kswapd进程确保内存空间总是在被释放中
     pdflush
          负责将内存中的内容和文件系统进行同步操作, 即写操作返回的时候数据并没有真正写到磁盘上, 而是先写到了系统的cache中, 随后又pdflush内核线程将系统中的脏页写到磁盘上
     内存信息
          cat /proc/meminfo
     --------------------------------------------------------------
     ps aux (RSS:实际占用的物理内存)
     ps -p javaProcessId -o rss 查看java进程内存使用情况
     --------------------------------------------------------------
     sar -r 内存和交换分区的使用率

磁盘IO
     df -ha 检查磁盘占用情况
     du -ha /XX(目录) 查看目录内档案占用磁盘大小
     iostat -x -d 1
     sar -b

Network
     ifconfig 网路配置信息
     ping
          TTL:IP数据包经过一个路由ttl减1
     netstat
          -a或-all: 显示所有连线的socket
          -n或-numeric: 直接使用IP, 不通过域名服务器
          -p或programs: 显示正在使用socket的程序识别码和程序名称
          -t或-tcp: 显示tcp连线状况
          netstat -anpt
     sar -n SOCK 查看网络链接资源
          totsck: Total number of sockets
          tcpsck: Number of tcp sockets cureently in use
     sar -n DEV 查看网络流量
     lsof 列出被进程打开的文件信息(比如调查too many open file 错误)
               lsof -p                     进程id
               lsof +D [目录]      目录下文件是否被某进程打开

其他:
     端口相连接的机器数:
          ss -nao | grep 18090 | wc -l
          netstat -anpt | grep 18090 | wc -l
     查看java线程:
          ps -eLf | grep java | wc -l
</code></pre><hr>
<p>jvm性能相关:<br>     java堆的经验值:<br>          Space                                Option                             Occupancy Factor<br>          Java heap                           -Xmx                               3x or 4x old generation space occupancy after full garbage collection<br>          Perm Generation                -MaxPermSize                1.2 or 1.5x perm generation space occupancy after full garbage collection<br>          Young Generation              -Xmn                               1x or 1.5x young generation space occupancy after full garbage collection<br>          Old Generation                                                           2x or 3x old generation space occupancy after full garbage collection</p>
<pre><code>jps（JVM Process Status Tools）
     jps [option] [hostid]
     其中hostid默认为本机，而option选项包含以下选项
          --------------------------------------------------------------
          Option    Function
          -q   只输出LVMID
          -m   输出JVM启动时传给主类的方法
          -l   输出主类的全名，如果是Jar则输出jar的路径
          -v   输出JVM的启动参数

jstat（JVM Statistics Monitoring Tools）
     jstat主要用于监控虚拟机的各种运行状态信息，如类的装载、内存、垃圾回收、JIT编译器等
     jstat [option vmid [interval [s|ms] [vount] ] ]
     参数interval和count分别表示查询间隔和查询次数，如每1毫秒查询一次进程20445的垃圾回收情况，监控20次，命令如下所示：
          jstat –gc 20445 1 20
          --------------------------------------------------------------
          Option    Function
          -class    监视类的装载、卸载数量以及类的装载总空间和耗费时间等
          -gc  监视Java堆，包含eden、2个survivor区、old区和永久带区域的容量、已用空间、GC时间合计等信息
          -gccapcity     监视内容与-gc相同，但输出主要关注Java区域用到的最大和最小空间
          -gcutil   监视内容与-gc相同，但输出主要关注已使用空间占总空间的百分比
          -gccause  与-gcutil输出信息相同，额外输出导致上次GC产生的原因
          -gcnew    监控新生代的GC情况
          -gcnewcapacity 与-gcnew监控信息相同，输出主要关注使用到的最大和最小空间
          -gcold    监控老生代的GC情况
          -gcoldcapacity 与-gcold监控信息相同，输出主要关注使用到的最大和最小空间
          -gcpermcapacity     输出永久带用到的最大和最小空间
          -compiler 输出JIT编译器编译过的方法、耗时信息
          -printcompilation   输出已经被JIT编译的方法

jinfo（JVM configuration Info for Java）
     Jinfo的作用是实时查看虚拟机的各项参数信息
     Jinfo [option] pid
          如 jinfo –sysprops {pid}

jmap（JVM Memory Map for Java）
     jmap用于生成堆快照（heapdump）。当然我们有很多方法可以取到对应的dump信息，如我们通过JVM启动时加入启动参数 –XX:HeapDumpOnOutOfMemoryError参数，可以让JVM在出现内存溢出错误的时候自动生成dump文件，亦可以通过-XX:HeapDumpOnCtrlBreak参数，在运行时使用ctrl+break按键生成dump文件，当然我们也可以使用kill -3 pid的方式去恐吓JVM生成dump文件。jmap的作用不仅仅是为了获取dump文件，还可以用于查询finalize执行队列、Java堆和永久带的详细信息，如空间使用率、垃圾回收器等。
     jmap [option] vmip
          --------------------------------------------------------------
          Option    Function
          -dump     生成对应的dump信息，用法为-dump:[live,]format=b,file={fileName}
          -finalizerinfo 显示在F-Queue中等待的Finalizer方法的对象（只在linux下生效）
          -heap     显示堆的详细信息、垃圾回收器信息、参数配置、分代详情等
          -histo    显示堆栈中的对象的统计信息，包含类、实例数量和合计容量
          -permstat 以ClassLoder为统计口径显示永久带的内存状态
          -F   当虚拟机对-dump无响应时可使用这个选项强制生成dump快照
          示例：jmap -dump:format=b,file=heap.dump 20445

jhat（JVM Heap Analysis Tool）
     jhat是用来分析dump文件的一个微型的HTTP/HTML服务器，它能将生成的dump文件生成在线的HTML文件，让我们可以通过浏览器进行查阅，然而实际中我们很少使用这个工具，因为一般服务器上设置的堆、栈内存都比较大，生成的dump也比较大，直接用jhat容易造成内存溢出，而是我们大部分会将对应的文件拷贝下来，通过其他可视化的工具进行分析
          jhat {dump_file}
               执行命令后，我们看到系统开始读取这段dump信息，当系统提示Server is ready的时候，用户可以通过在浏览器键入http://ip:7000进行查询。

jstack（JVM Stack Trace for java）
     jstack用于JVM当前时刻的线程快照，又称threaddump文件，它是JVM当前每一条线程正在执行的堆栈信息的集合。生成线程快照的主要目的是为了定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部时长过长导致线程停顿的原因。通过jstack我们就可以知道哪些进程在后台做些什么？在等待什么资源等！
     jstack [option] vmid
          --------------------------------------------------------------
          Option    Function
          -F   当正常输出的请求不响应时强制输出线程堆栈
          -l   除堆栈信息外，显示关于锁的附加信息
          -m   显示native方法的堆栈信息
          示例：jstack -l 20445

查找占用cpu高的java线程:
     top -H -p javaId                        // 找出java thread id
     printf &apos;0x%x\n&apos; java thread id          // 转16进制
     jstack
</code></pre><hr>
<pre><code>S0C：年轻代中第一个survivor（幸存区）的容量 (字节) 
     S1C：年轻代中第二个survivor（幸存区）的容量 (字节) 
     S0U：年轻代中第一个survivor（幸存区）目前已使用空间 (字节) 
     S1U：年轻代中第二个survivor（幸存区）目前已使用空间 (字节) 
     EC：年轻代中Eden（伊甸园）的容量 (字节) 
     EU：年轻代中Eden（伊甸园）目前已使用空间 (字节) 
     OC：Old代的容量 (字节) 
     OU：Old代目前已使用空间 (字节) 
     PC：Perm(持久代)的容量 (字节) 
     PU：Perm(持久代)目前已使用空间 (字节) 
     YGC：从应用程序启动到采样时年轻代中gc次数 
     YGCT：从应用程序启动到采样时年轻代中gc所用时间(s) 
     FGC：从应用程序启动到采样时old代(全gc)gc次数 
     FGCT：从应用程序启动到采样时old代(全gc)gc所用时间(s) 
     GCT：从应用程序启动到采样时gc用的总时间(s) 
     NGCMN：年轻代(young)中初始化(最小)的大小 (字节) 
     NGCMX：年轻代(young)的最大容量 (字节) 
     NGC：年轻代(young)中当前的容量 (字节) 
     OGCMN：old代中初始化(最小)的大小 (字节) 
     OGCMX：old代的最大容量 (字节) 
     OGC：old代当前新生成的容量 (字节) 
     PGCMN：perm代中初始化(最小)的大小 (字节) 
     PGCMX：perm代的最大容量 (字节)   
     PGC：perm代当前新生成的容量 (字节) 
     S0：年轻代中第一个survivor（幸存区）已使用的占当前容量百分比 
     S1：年轻代中第二个survivor（幸存区）已使用的占当前容量百分比 
     E：年轻代中Eden（伊甸园）已使用的占当前容量百分比 
     O：old代已使用的占当前容量百分比 
     P：perm代已使用的占当前容量百分比 
     S0CMX：年轻代中第一个survivor（幸存区）的最大容量 (字节) 
     S1CMX ：年轻代中第二个survivor（幸存区）的最大容量 (字节) 
     ECMX：年轻代中Eden（伊甸园）的最大容量 (字节) 
     DSS：当前需要survivor（幸存区）的容量 (字节)（Eden区已满） 
     TT： 持有次数限制 
     MTT ： 最大持有次数限制 
</code></pre><hr>
<p>其他</p>
<p>sudo -u admin /opt/taobao/java/bin/jstat -gcold <code>ps aux | grep tomcat | awk &#39;{ if($1==&quot;admin&quot;) print $2}&#39;</code> 1000 –&gt;gc</p>
<p>sudo -u admin /opt/taobao/java/bin/jstack -l <code>ps aux | grep tomcat | awk &#39;{ if($1==&quot;admin&quot;) print $2}&#39;</code> &gt; /tmp/js5.log –&gt;jvm_stack</p>
<p>sudo -u admin /opt/taobao/java/bin/jstat -gcutil <code>ps aux | grep tomcat | awk &#39;{ if($1==&quot;admin&quot;) print $2}&#39;</code> 1000 –&gt;gc老年代</p>
<p>top -Hp pid<br>printf ‘%x\n’ pid<br>jstack 10765 | grep ‘0x2a34’ -C5 –color</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/08/分类-线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/分类-线性模型/" itemprop="url">分类-线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T10:18:01+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<p><strong>简介</strong><br></p>
<h2 id="主要减少两种很流行但是不同的方法，对数模型、logit模型：线性判别分析和线性逻辑回归。增广空间中的线性函数映射到原空间中的二次函数，即线性决策边界对应二次决策边界。"><a href="#主要减少两种很流行但是不同的方法，对数模型、logit模型：线性判别分析和线性逻辑回归。增广空间中的线性函数映射到原空间中的二次函数，即线性决策边界对应二次决策边界。" class="headerlink" title="主要减少两种很流行但是不同的方法，对数模型、logit模型：线性判别分析和线性逻辑回归。增广空间中的线性函数映射到原空间中的二次函数，即线性决策边界对应二次决策边界。"></a>主要减少两种很流行但是不同的方法，对数模型、logit模型：线性判别分析和线性逻辑回归。增广空间中的线性函数映射到原空间中的二次函数，即线性决策边界对应二次决策边界。</h2><p>LDA与逻辑回归最大的区别是一个是判别模型一个是生成模型。监督机器学习方法可以分为<strong>生成方法和判别方法。</strong> 常见的生成方法有混合高斯模型、朴素贝叶斯法、隐形马尔科夫模型，常见的判别方法有SVM、LR。生成方法学习出的是生成模型，判别方法学习出的是判别模型。<strong>数据直接学习决策函数Y=f(x)或条件概率分布P(Y|X)得到的预测模型就是判别模型</strong>，即判别方法关心的是对于给定的输入X应该预测什么样的输出Y。 <strong>由数据学习联合概率分布P(X,Y)，然后由P(Y|X)=P(X,Y)/P(X)求出概率分布P(Y|X)作为预测模型，该方法表示了给定输入X与产生输出Y的生成关系。</strong> 生成模型：朴素贝叶斯、隐马尔科夫；判别模型：knn、感知机、决策树、逻辑回归、线性回归、最大熵模型、SVM、提升、条件随机场<br></p>
<p> <strong>定义训练数据(C,X)，$$C={c_1,c_2,…,c_n}$$是哪个训练样本的label,$$X={x_1,x_2,…,x_n}$$是n个样本的特征。定义单个测试数据$$(\hat{c},\hat{x})$$,$$\hat{c}$$为测试数据label,$$\hat{x}$$是测试样本的特征。</strong></p>
<ul>
<li>判别模型求解思路：条件分布$$\rightarrow$$  模型参数后验概率最大$$\rightarrow$$ (似然函数 参数先验)最大$$\rightarrow$$最大似然</li>
<li>给定输入$$\hat{x}$$,生成模型可以给出输入和输出的联合分布$$P(\hat{x},\hat{c})$$，所以生成方法的目标是求出这个联合分布。以朴素贝叶斯为例：联合分布$$\rightarrow$$求解类别先验概率和类别条件概率。</li>
<li>生成模型可以还原联合概率分布，判别不行；生成方法的学习收敛更快，样本容量增加时，学到的模型可以更快的收敛于真实模型。存在隐变量时，仍可以利用生成方法学习，判别方法不能用；判别学习不能反应训练数据本身的特性，但它寻找最优分类面，反映的是异类数据之间的差异，直接面对预测，往往学习的准确率较高，由于直接学习P(Y|X)或Y=f(X)从而可以简化学习；生成模型是从大量数据中找规律，属于统计学习；而判别模型只关心不同类型数据的差别，利用差别来分类。<br></li>
</ul>
<hr>
<p><strong>LDA</strong>线性判别分析<br><br>很多技术都基于类密度模型：</p>
<ul>
<li>线性和二次判别分析利用高斯密度</li>
<li>更灵活的混合高斯模型允许非线性的决策边界</li>
<li>非参数密度估计允许最大的灵活性</li>
<li>朴素贝叶斯假设类别条件先验概率相互独立</li>
</ul>
<p>对于贝叶斯公式而言，最重要的就是类条件概率$$f_k(x)$$,很多算法之所以不同就是这个类条件概率密度函数的参数形式的假设不同，比如：<br></p>
<ul>
<li>线性判别分析LDA，假设$$f_k(x)$$均值不同，方差相同的高斯分布；</li>
<li>二次判别分析QDA假设其均值、方差均不同；</li>
<li>高斯混合模型GMM假设$$f_k(x)$$是不同的高斯分布组合</li>
<li>很多非参数方法假设其实参数的密度函数，比如直方图</li>
<li>朴素贝叶斯假设其是$$C_k$$边缘密度函数，即类别之间是独立同分布的</li>
</ul>
<p>各种算法的不同，基本上都是来至于对条件概率密度函数不同。<br><br>贝叶斯公式为：<br><br>$$Pr(G=k|X=x)=\tfrac{f_k(x)\pi_k}{\sum_{l=1}^Kf_l(x)\pi_l}$$<br><br>因为LDA假设$$f_k(x)$$均值不同，方差相同的高斯分布，所以其类条件概率密度函数为：<br><br>$$f_k(x)=\tfrac{1}{(2\pi)^{p/2}|\sum|^{1/2}}exp(-\frac{1}{2}(x-u_k)^T\sum^{-1}(x-u_k))$$<br><br>特征x的维度为p维，类别$$C_k$$均值为$$u_k$$，所有类别方差为$$\sum$$，对于二分类问题有：<br><br>$$log\frac{P(C_1|x)}{P(C_2|x)}=log\frac{f_1(x)}{f_2(x)}+log\frac{\pi_1}{\pi_2}$$<br><br>因此可以得到决策函数或判别函数为：<br><br>$$\delta_k(x)=x^T\sum^{-1}u_k-\frac{1}{2}u_k^T\sum^{-1}u_k+log\pi_k$$<br><br>其中的参数是从数据中估计出来的：</p>
<ul>
<li>$$u_k=N_k/N,N_k$$是类别Ck的样本数，N是样本总数</li>
<li>$$u_k=\frac{1}{N_k}\sum_{x\in C_k}x$$就是类别Ck的均值</li>
<li>$$\sum=\frac{1}{N-K}\sum_{k=1}^K\sum_{x\in C_k}(x-u_k)(x-u_k)^T$$<br><br><strong>二次判别分析QDA</strong><br><br>均值、方差都不同的高斯分布，由于$$\sum_k$$不一样，所以其二次项存在，其决策面为：<br><br>$$log\frac{P(C1|x)}{P(C2|x)}=log\frac{f_1(x)}{f_2(x)}+log\frac{\pi_1}{\pi_2}=x^T(\sum_1^{-1}-\sum_2^{-1})x+x^T\sum^{-1}(\mu_1-\mu_2)-\frac{1}{2}(\mu_1+\mu_2)^T\sum^{-1}(\mu_1-\mu_2)+log\frac{\pi_1}{\pi_2}-1/2log\frac{\sum_1}{\sum_2}$$<br><br>其对应判别函数为：<br><br>$$\delta_k(x)=-1/2(x-\mu_k)^T\sum^{-1}(x-\mu_k)-1/2log|\sum_k|+log\pi_k$$<br><br><strong>Fisher判别式</strong><br><br>从贝叶斯公式出发，得到线性判别分析公式，也可以从另一个角度来看线性判别分析，就是Fisher判别式，Fisher判别式就是线性判别分析LDA，Fisher判别式更侧重于LDA的数据降维的能力。<br><br>线性判别的基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，<strong>投影后保证模式样本在新的子空间有最大类间距离和最小的类内距离。</strong> 即模式在该空间有最佳的可分离性，因此它是一种有效的特征抽取方式。<br><br>QDA与LDA相同的是也假设每个类的观测值都来自于高斯分布，与LDA不同的是QDA假设每个类都有它们自己的协方差矩阵。<br><br>LDA需要(k-1)(p+1)个参数，KDA需要(k-1)(p(p+3)/2+1)个参数。为什么LDA和QDA的分类性能比较好，因为这些数据只支持简单的线性和二次决策边界。<br></li>
<li>真实分类是线性时，LR和LDA通常表现更好，真实边界是一般的非线性时，QDA表现更好，当真实的分类边界更复杂时，KNN会表现更好。</li>
<li>LR和LDA都将产生线性决策边界，两种唯一的不同是LR的系数估计是通过极大似然估计，LDA的系数是运用正态分布的均值和方差的估计值得到的；LR适合二分类问题，对于多分类问题，LDA更常见；</li>
<li>样本能够完全线性分类时，LR参数估计会出现不稳定情况，LR方法不适合。LDA参数估计不会出现这种情况；样本量较小且自变量分布近似于高斯分布，LDA比LR表现更好；</li>
<li>LDA和QDA都是建立在自变量服从正态分布的假设上，当自变量的分布确实是几乎服从正态分布时，两种方法表现的较好；选择LDA和QDA的关键在于偏置-方差权衡。</li>
<li>QDA可以认为是非参数学习法KNN和具有线性分类边界LR及LDA方法的折中，QDA假设分类边界是二次的，能够比线性模型更准确的为更多问题建立模型；同时由于其二次边界的额外假设，样本量较小时，其能够优于KNN方法。</li>
<li>KNN是非参数学习方法，不对分类边界做任何假设，分类边界为高度非线性便会优于LR和LDA，但是其不能反映每个自变量对因变量的影响</li>
<li>LR可以适用于含有定性自变量的模型，对于定性的自变量LR会将其转换为虚拟变量，但是LDA QDA KNN都不适用于定性的自变量。</li>
<li>LDA容忍部分误差，为了减少方差。<br><br><strong>RDA Regularization Discriminant Analysis</strong><br><br>RDA是LDA和QDA的一个折中，把各个协方差矩阵转为一个统一的矩阵。这个方法和岭回归有些类似：<br><br>$$\hat{\sum_k}(\alpha)=\alpha\hat{\sum_k}+(1-\alpha)\hat{\sum}$$<br><br>$$\hat{\sum}$$是LDA中合并的协方差矩阵<br><br><strong>降秩线性判别</strong><br><br>可以把LDA看作是一个作限制的高斯分类器。之所以受欢迎是因为有一个额外的限制,可以查看数据的低维投影。p维数据有K个分类，假如p比k大的多，可能就要降维处理，舍去某些属性。在确定最近的中心时，可以舍去与子空间正交的距离，因为对每个类贡献的相同。<br><br>费舍尔问题本质上就是最大化瑞利商。<br></li>
</ul>
<hr>
<p><strong>逻辑回归</strong><br><br>逻辑回归来源于在x中通过线性函数来模拟k分类的后验概率的愿望，同时保证概率加起来为1，范围是0-1之间。模型像这种形式：<br><br>$$log\frac{Pr(G=1|X=x)}{Pr(G=K|X=x)}=\beta_{10}+\beta_1^Tx$$<br><br>$$log\frac{Pr(G=K-1|X=x)}{Pr(G=K|X=x)}=\beta_{(K-1)0}+\beta_K^Tx$$<br><br>尽管这个模型使用最后一个类做分母，但是这个选择是任意的，这个和下面的的等价：<br><br>$$Pr(G=k|X=x)=\tfrac{exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+\beta_l^Tx)},k=1,…,K-1$$<br><br>$$Pr(G=K|X=x)=\tfrac{1}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+\beta_l^Tx)}$$<br><br>为了强调对参数集的依赖性$$\theta={\beta_{10},\beta_1^T,…,\beta_{(K-1)0},\beta_{K-1}^T}$$<br></p>
<h2 id="表示概率：-Pr-G-k-X-x-p-k-x-theta"><a href="#表示概率：-Pr-G-k-X-x-p-k-x-theta" class="headerlink" title="表示概率：$$Pr(G=k|X=x)=p_k(x;\theta)$$"></a>表示概率：$$Pr(G=k|X=x)=p_k(x;\theta)$$<br></h2><p><strong>二次近似推断</strong><br><br><strong>近似推断 变分推断 KL散度 平均场mean field</strong><br><br>概率推断你的核心任务就是计算某分布下某个函数的期望、或者计算边缘概率分布、条件概率分布。EM算法中，就是利用对数似然函数在隐变量后验分布下的期望。这些往往需要积分或求和操作，但这些操作不是那么容易，积分中可能有很复杂的形式，无法直接求解。如果分布是类似指数族分布这样具有共轭分布、容易得到解析解的分布形式。但是如果变量空间有很高的维度，这样做数值积分就不太可能。这两个原因导致进行精确计算往往是不可能的。<br><br>近似计算有随机和确定两条路子。随机方法就是MCMC之类的采样法。确定近似法就是变分。变分法的优点主要：有解析解、计算开销小、易于在大规模问题中应用。缺点：推导出想要的形式比较困难。可以在不同的场合应用变分法或采样法。变分法相当于把微积分从变量推广到函数上了。<br><br>这里是用高斯分布去近似目标分布的极值点。也叫拉普拉斯近似。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/08/线性回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/线性回归/" itemprop="url">线性回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T10:15:58+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p>令$$X^T=(X_1,X_2,…,Xp)$$,单独的X可以看做列向量。用y1代表预测值，y代表观测值。利用最小二乘法求$$f(X)=\beta_0+\sum_{j=1}^pXj\beta_j=X^T\beta+\epsilon$$<br></p>
<p>利用残差平方和衡量误差:$$RSS(\beta)=\sum_{i=1}^N(y_i-f(x_i))^2$$<br></p>
<p>假设N是一个<strong>Nx(P+1)</strong> 的矩阵，则可得到：<br><br>$$arg\ minRSS(\beta)=(y-X\beta)^T(y-X\beta)$$<br></p>
<p>求导得到$$\tfrac{\partial RSS}{\partial \beta}=-2X^T(y-X\beta)$$<br></p>
<p>$$\tfrac{\partial^2 RSS}{\partial \beta \partial \beta^T}=2X^TX$$<br><br>加入X是列满秩矩阵，则$$X^TX$$是正定的，可以得到<br><br>$$X^T(y-X\beta)=0$$ (3.5)<br></p>
<p>得到$$\hat{\beta}=(X^TX)^{-1}X^Ty$$<br></p>
<p>因此$$y1=X\beta=X(X^TX)^{-1}X^Ty$$<br></p>
<p>令$$H=X(X^TX)^{-1}X^T$$,把H称作投影矩阵。</p>
<p><strong>为什么X是p维的，后面会变成p+1</strong><br></p>
<h2 id="因为模型转变为-f-x-X-T-beta-去掉了-beta-0-那一项，所以X加了一列全是1的。"><a href="#因为模型转变为-f-x-X-T-beta-去掉了-beta-0-那一项，所以X加了一列全是1的。" class="headerlink" title="因为模型转变为$$f(x)=X^T\beta$$去掉了$$\beta_0$$那一项，所以X加了一列全是1的。"></a>因为模型转变为$$f(x)=X^T\beta$$去掉了$$\beta_0$$那一项，所以X加了一列全是1的。</h2><p><strong>证明$$Var(\beta)=(X^TX)^{-1}\sigma^2$$</strong><br><br>$$E(y)=X\beta$$,可以得到<br><br>$$E(\hat{\beta})=(X^TX)^{-1}X^TX\beta$$</p>
<p>$$\hat{\beta}-E(\hat{\beta})=(X^TX)^{-1}X^Ty-(X^TX)^{-1}X^TX\beta=(X^TX)^{-1}X^T(y-X\beta)=(X^TX)^{-1}X^T\epsilon$$<br><br>$$Var(\hat{\beta})=E[(\hat{\beta}-E(\hat{\beta}))(\hat{\beta}-E(\hat{\beta}))^T]=(X^TX)^{-1}X^TE(\epsilon \epsilon)^TX(X^TX)^{-1}=(X^TX)^{-1}X^TVar(\epsilon)X(X^TX)^{-1}$$<br></p>
<p>假设y是不相关的，方差为$$\sigma^2$$，所以有$$Var(\epsilon)=\sigma^2I_N$$,其中$$I_N$$是一个NxN的单位矩阵。所以就有:<br></p>
<h2 id="Var-hat-beta-sigma-2-X-TX-1-X-TX-X-TX-1-X-TX-1-sigma-2"><a href="#Var-hat-beta-sigma-2-X-TX-1-X-TX-X-TX-1-X-TX-1-sigma-2" class="headerlink" title="$$Var(\hat{\beta})=\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}=(X^TX)^{-1}\sigma^2$$"></a>$$Var(\hat{\beta})=\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}=(X^TX)^{-1}\sigma^2$$</h2><p><strong>证明$$\hat{\sigma^2}=\tfrac{1}{N-p-1}\sum_{i=1}^N(y_i-\hat{y_i})^2$$为$$\sigma^2$$的无偏估计</strong><br></p>
<p>假设x服从标准正态分布。由上面3.5式可得残差向量$$y-\hat{y}$$和R的子空间$$X^T$$正交，所以$$y-\hat{y}$$的维度是N-p-1，且服从$$N(0,\sigma^2I_{N-p-1})$$分布。因此$$E(\sum_{i=1}^N(y_i-\hat{y_i})^2)=(N-p-1)\sigma^2$$,于是就得到：<br>$$E(\hat{\sigma^2})=\sigma^2$$,即$$\hat{\sigma^2}$$是$$\sigma^2$$的无偏估计。</p>
<hr>
<p><strong>高斯-马尔科夫定理</strong><br><br>在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量，即BLUE(best linear unbiased estimator)，条件是误差不相关性。<br><br>假设X是固定的，有一个线性输出函数$$c_0^Ty$$，假设这个线性模型是正确的，则$$a^T\hat{\beta}$$是无偏的，因为$$E(a^T\beta)=E(a^T(X^TX)^{-1}X^Ty)=a^T(X^TX)^{-1}X^TX\beta=a^T\beta$$<br><br>高斯马尔科夫理论表明加入有另一个线性估计：$$\hat{\theta}=c^Ty$$是$$a^Ty$$的无偏估计，即$$E(c^Ty)=a^T\beta $$根据高斯马尔科夫定理：<br><br>$$Var(a^T\hat{\beta})&lt;=Var(c^Ty)$$<br><br>利用三角不等式证明：<br><br>设b是一个长度为N的列向量，$$E(b^Ty)=\alpha^T\beta$$，b值是固定的，等式对于所有的$$\beta$$都成立，假设X不是随机的，于是$$E(b^Ty)=b^TX\beta$$，于是$$b^TX=\alpha^T$$,又得到：$$Var(\alpha^T\hat{\beta})=\alpha^T(X^TX)^{-1}\alpha=b^TX(X^TX)^{-1}X^Tb$$，$$Var(b^Ty)=b^Tb$$，所以我们要证明的是$$X(X^TX)^{-1}X^T&lt;=I_N$$<br><br>为了证明，引入QR分解，X=QR,Q具有正交列NxP<br>,R是一个p*p的上三角值，且值是正的。列满秩矩阵一定可以QR分解。所以$$X^TX=R^TQ^TQR=R^TR$$，因此$$X(X^TX)^{-1}X^T=QQ^T$$，设[QQ1]是一个正交矩阵(N,N)。因此$$I_N=[Q\ Q_1] [Q^T\ Q_!^T]=QQ^T+Q_1Q_1^T$$<br></p>
<p>估计参数$$\theta$$的均方差：<br><br>$$MSE(\hat{\theta})=E(\hat{\theta}-\theta)^2=E[(\hat{\theta}-E(\hat{\theta}))^2+2(\hat{\theta}-E(\hat{\theta}))(E(\hat{\theta})-\theta)+(E(\hat{\theta})-\theta)^2]=Var(\hat{\theta})+2(E(\hat{\theta})-E(\hat{\theta}))(E(\hat{\theta})-\theta)+(E(\hat{\theta})-\theta)^2=Var(\hat{\theta})+(E(\hat{\theta})-\theta)^2$$<br><br>一个模型的误差可以写为:<br><br>$$E(Y_0-f(x_0))^2=\sigma^2+MSE(f(x_0))$$<br></p>
<p>在无偏估计的线性模型中，最小二乘的平方误差最小。或许存在有偏估计有更小的平方误差呢，这样一个估计可以损失一点偏差，但是可以减小方差。有偏估计也是常用的。任何把最小二乘参数缩小到0的方法都可能导致有偏估计，包括变量子集选择、岭回归。从更实际的角度看，大多数模型都是无偏估计的变形，因此是有偏的。模型的选择就是偏差和方差的均衡。<br></p>
<hr>
<p><strong>简单变量回归到多元回归</strong><br>由于矩阵维数增加后，求逆比较麻烦引入斯密特正交化方法，进而再利用QR分解，使$$X=QR$$，Q是一个Nxp的列正交矩阵，$$Q^TQ=I$$；R是(p+1)x(p+1)的上三角矩阵。QR分解是X的一个正交基。通过这种方式很显然：<br><br>$$\hat{\beta}=(X^TX)X^Ty=(R^TR)^{-1}R^TQ^Ty=R^{-1}R^{-T}R^TQ^Ty=R^{-1}Q^Ty$$<br><br>$$\hat{y}=X\hat{\beta}=QRR^{-1}Q^Ty=QQ^Ty$$</p>
<hr>
<p><strong>子集选择</strong><br><br>不满意最小二乘的两点：<br><br>1、预测精度。最小二乘是低偏差、高方差。所以我们可以牺牲一些偏差，降低一些方差。<br><br>2、解释性。参数过多，解释性就不强。我们可以选择更小的子集，忽略一些细节。<br><br><strong>下面介绍几种变量子集选择</strong><br><br><strong>A、最佳子集选择</strong><br><br>其实就是偏差和方差的权衡，选哪些维度。有大量的准则可以用，比如AIC。<br><br><strong>B-1、向前  向后逐步选择</strong><br><br>相比最佳子集选择是要从所有的子集里选择，向前逐步选择是从截距开始选择，连续的选择使模型最优的维度。但是你可能会认为会有很大的计算量，但是QR分解会使计算相当方便。向前逐步选择主要有两大有点：<br><br>（1）计算量上，当p&gt;&gt;N，仍然是可以选择这个子集的；<br><br>（2）统计量上，经常会有一个更高的偏差，和更低的方差。<br><br><strong>B-2、后向逐步选择</strong><br><br>这个算法是从所有子集里面选择，连续的删掉一些对模型影响最小的维度，这种方法只适用于$$p&lt;N$$，而前向逐步回归都适用。这几种表现都很相似，前向分段回归会花更长的时间达到最小误差。<br>一些软件包中会提供前向后向的集成方法,比如某一步会选择增加一个项或删去一个项，选择这两者中最好的，比如依照AIC准则，或者依据F统计量(过时了，因为这个统计量不能衡量多种统计量)<br><br><strong>C、向前分段回归</strong><br></p>
<h2 id="适用于-p-lt-N-，forward-stagewise-FS比前向逐步回归约束更多，像前向逐步回归一样，从截距-overline-y-开始，以系数为中心，初始化为0。不像前向逐步回归，当加入一项时，其他变量不会更改。因此FS花费的步骤要大于p，因此高维度下，会出现拟合很慢的情况。"><a href="#适用于-p-lt-N-，forward-stagewise-FS比前向逐步回归约束更多，像前向逐步回归一样，从截距-overline-y-开始，以系数为中心，初始化为0。不像前向逐步回归，当加入一项时，其他变量不会更改。因此FS花费的步骤要大于p，因此高维度下，会出现拟合很慢的情况。" class="headerlink" title="适用于$$p&lt;N$$，forward-stagewise FS比前向逐步回归约束更多，像前向逐步回归一样，从截距$$\overline{y}$$开始，以系数为中心，初始化为0。不像前向逐步回归，当加入一项时，其他变量不会更改。因此FS花费的步骤要大于p，因此高维度下，会出现拟合很慢的情况。"></a>适用于$$p&lt;N$$，forward-stagewise FS比前向逐步回归约束更多，像前向逐步回归一样，从截距$$\overline{y}$$开始，以系数为中心，初始化为0。不像前向逐步回归，当加入一项时，其他变量不会更改。因此FS花费的步骤要大于p，因此高维度下，会出现拟合很慢的情况。<br></h2><p><strong>缩减方法</strong><br><br>这里也称作特征缩减技术<br><br>通过保留部分子集舍去剩下的，只选择部分子集比全部数据集，会更容易理解、低误差,这是一个离散过程，一些变量要么被保留要么被舍弃，表现出高方差;方法缩减更加连续，不会有高变异性。<br><br><strong>Lasso：L1正则化</strong><br><br><strong>Ridge：L2正则化</strong><br><br><strong>1、Ridge Regression岭回归</strong><br><br>岭回归通过增加惩罚系数，来收缩回归系数。岭系数最小化残差平方和。$$\hat{\beta}<em>{ridge}=argmin</em>{\beta}{\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}$$<br><br>$$\lambda$$是一个控制收缩量的复杂参数，$$\lambda$$越大，收缩量越大，甚至可以缩减至0.通过残差平方作为惩罚项也被用在神经网络中，被称为权重衰减。一个相等的方式也可写为：<br><br>$$\hat{\beta}^{ridge}=argmin_{\beta}\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2,$$<br><br>条件：$$\sum_{j=1}^N\beta_j^2\leq t$$<br><br>当线性模型参数比较多时，参数就无法确定，表现出高方差。一些变量上比较大的参数可以和相关的变量负参数抵消，通过控制$$\lambda$$的大小，这个问题得到缓解。第一步先将输入的数据规范化，注意到惩罚项对截距不起作用，可以令$$x_{ij}=x_{ij}-\overline{x}$$，令$$\beta_0=\tfrac{1}{N}\sum_1^Ny_i$$,因此X是p维(列)。<br><br>$$RSS()=(y-X\beta)^T(y-X\beta)+\lambda\beta^T\beta$$<br><br>因此岭回归的解可以很容易得到：<br><br>$$\hat{\beta}^{ridge}=(X^TX+\lambda I)^{-1}X^Ty$$<br><br>I是一个(p,p)的矩阵，从公式可以看出仍然是关于y的线性函数，这就使得这个问题变成非奇异问题。在标准正交输入的情况下，岭回归只是最小二乘的一个标度形式，$$\hat{\beta}^{ridge}=\hat{\beta}/(1+\lambda)$$<br><br><strong>扩展</strong><br></p>
<ol>
<li>least square解析解可以用Gaussian分布和最大似然估计求得；</li>
<li>ridge回归可以用Gaussian分布和最大后验估计解释；</li>
<li>lasso回归可以用拉普拉斯分布和最大后验估计解释。<br></li>
</ol>
<p>岭回归可以推导为后验均值，利用一个合适的先验分布。假设$$y_i\in N(\beta_0+x_i^T\beta,\sigma^2)$$,参数$$\beta_j$$互相独立服从$$N(0,\tau^2)$$,利用对数后验密度，得到$$\lambda=\tfrac{\sigma^2}{\tau^2}$$因此岭回归是后验分布模式，因为是高斯分布，也叫做后验均值。<br></p>
<p><strong>posterior mean:</strong> 利用MSE作为风险项，利用贝叶斯对未知参数进行估计，就称作后验分布的均值：$$\hat{\theta}(x)=E(\theta|x)=\int \theta p(\theta|x)d\theta$$<br><br>可以利用SVD(singular value decomposition)对输入矩阵X获得额外的见解，$$X=UDV^T$$,U(N,p)和V(p,p)都是正交矩阵,D(p,p)对角矩阵，$$d_1\geq d_2 \geq d_3 \geq …\geq d_p\geq 0$$,叫做X的奇异值，如果有一个或更多$$d_j=0$$，X就是奇异的。因此最小二乘可以写为：$$X^TX=VDU^TUDV^T=VD^2V^T$$,<br></p>
<p>$$X\hat{\beta}^{ls}=X(X^TX)^{-1}X^Ty=UDV^T(VD^2V^T)^{-1}VDU^Ty=UDV^T(V^{-T}D^{-2}V^{-1})VDU^Ty=UU^Ty=\sum_{j=1}^pu_j(u_j^Ty)$$<br></p>
<p>岭回归可以写为：<br><br>$$\hat{\beta}^{ridge}=X(X^TX+\lambda I)^{-1}X^Ty=(VD^2V^T+\lambda VV^T)^{-1}VDU^Ty=(V(D^2+\lambda I)V^T)^{-1}VDU^Ty=V(D^2+\lambda I)^{-1}DU^Ty$$</p>
<p>因此：<br><br>$$X\hat{\beta}^{ridge}=UD(D^2+\lambda I)^{-1}DU^Ty=\sum_{j=1}^pu_j\tfrac{d_j^2}{d_j^2+\lambda}u_j^Ty$$,其中$$u_j$$是U的列。对比QR分解$$\hat{y}=QQ^Ty$$和SVD分解$$\hat{y}=UU^Ty$$，Q和U是X的列的不同正交基。<br></p>
<h2 id="d-j-2-越小惩罚力度越大。SVD其实是PCA的另一种展示方式。"><a href="#d-j-2-越小惩罚力度越大。SVD其实是PCA的另一种展示方式。" class="headerlink" title="$$d_j^2$$越小惩罚力度越大。SVD其实是PCA的另一种展示方式。"></a>$$d_j^2$$越小惩罚力度越大。SVD其实是PCA的另一种展示方式。</h2><p><strong>关于有效自由度的讨论</strong><br><br>$$df(\lambda)=tr[X(X^TX+\lambda I)^{-1}X^T]=tr(H_{\lambda})=\sum_{j=1}^p\tfrac{d_j^2}{d_j^2+\lambda}$$<br><br>$$\lambda$$的单调递减函数是回归拟合的有效自由度，$$\lambda=0,df(\lambda)=p$$,即不正则化处理。其实总是会有一个额外的度，但X中心化处理后，被移除了。我们可以利用这个表达式来确定一个值，利用它应用交叉验证。通过$$\lambda$$保证所有可能的正则线性模型都有精确地覆盖。令$$df(\lambda)=k$$，k=1,2,3…,p代表了所有可能的自由度，利用牛顿法去求解$$\lambda$$。<br><br>$$\sum_{i=1}^p=\tfrac{d_j^2}{d_j^2+\lambda}=k$$, $$d(\lambda)=\sum_{i=1}^p=\tfrac{d_j^2}{d_j^2+\lambda}-k$$<br><br>想利用$$d(\lambda)=0$$,从而求出$$\lambda$$，利用牛顿法迭代<br><br>$$\lambda_{n+1}=\lambda_n-\frac{d(\lambda_n)}{d’(\lambda_n)}$$<br></p>
<p>$$d’(\lambda)=-\sum_{i=1}^p\frac{d_j^2}{(d_j^2+\lambda)^2}$$ 给$$\lambda_0$$一个初始值</p>
<hr>
<p><strong>Lasso</strong><br><br>公式定义为:<br><br>$$\hat{\beta}^{lasso}=argmin\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2$$,subject to $$\sum_{j=1}^p|\beta_j|\leq t$$<br><br>在信号处理中，被称为基追踪。<br></p>
<p>$$\hat{\beta}^{lasso}=argmin{\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|}$$<br></p>
<p>t如果很小，有些$$\beta$$就会为0，因此lasso就是连续子集选择。</p>
<hr>
<p><strong>子集选择 岭回归  lasso回归</strong><br><br>在输入数据X是正交矩阵的情况下，这三种方法都有明确的方法，都是最小二乘的一个简单变形。岭回归做的是一个比例收缩，lasso是利用常数$$\lambda$$来衡量每一个系数，在0处截断，也被称为软阈值法(soft thresholding)。子集选择只是选择M维数据中的部分，也就是硬阈值hard-thresholding，没有软阈值那么常见，可能因为硬阈值解决的问题是非凸问题。<br><br>$$n_s(w,\lambda)=w+\lambda,(w\leq -\lambda);0,|w|\leq\lambda;w-\lambda,w\geq\lambda$$<br><br>软阈值用来解决如下优化问题：<br><br>$$argmin|X-B|^2+\lambda ||X||_1,i\in(1,n)$$</p>
<p>硬阈值:$$\eta_H(w,\lambda)=w,|w|\geq\lambda;0,|w|\leq\lambda$$<br><br>用于解决：<br><br>$$argmin|X-B|^2+\lambda||X||_0$$<br><br>$$|X|_0$$是求向量X的零范数，$$|x|_0=1,x!=0;0,x=0$$<br></p>
<ul>
<li>best subset:  $$\beta_j*I[rank(|\hat{\beta}_j|\leq M)]$$</li>
<li>Ridge: $$\hat{\beta}/(1+\lambda)$$</li>
<li>Lasso: $$sign(\hat{\beta}_j)(|\hat{\beta}<em>j|-\lambda)</em>+$$ <br></li>
</ul>
<p><strong>稀疏与正则约束 ridge lasso</strong><br><br>通常为了推导的简洁性会加上1/2，变为:<br><br>$$J_R(w)=\frac{1}{2}||y-Xw||^2+\frac{\lambda}{2}||w||^2$$<br><br>上述求得的就是参数$$\beta$$的最优解<br><br>ridge实际上就是做了个缩减，而lasso实际上是做了一个soft thresholding，把很多权重项置为0，所以就得到了稀疏结果。<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/WEBRESOURCE44cb57c3de0d5a86fea2a23ea1bccac2/4445" alt="image"></p>
<p><strong>范数与稀疏矩阵</strong><br><br><strong>L-P范数</strong><br><br>与闵可夫斯基距离定义一样，L-P范数不是一个范数而是一组范数，定义如下:<br><br>$$L_p=p\sqrt{\sum_1^nx_i^p},x=(x1,x2,…,xn)$$</p>
<p>ridge regression是对权重w(或$$\beta$$)做L2范式约束，把解约束把在一个l2-ball里面，放缩是对球的半径放缩，因此w的每一个维度都在以同一个系数放缩，通过放缩不会产生稀疏的解，即某些w的维度是0.而实际应用中，数据的维度中是存在噪声和冗余的，稀疏的解可以找到有用的维度并减少冗余，提高回归预测的准确性和鲁棒性(减少overfitting)，在压缩感知、稀疏编码等非常多的机器学习模型中都需要用到稀疏约束。<br></p>
<p>稀疏约束最直观的形式应该是L0范式，很明显w的0范式是求w中非0元素的个数，而且L0是非凸的，在线性回归中，加上L0范式约束就变成了一个组合优化问题，这是一个NP问题。但是L1范式也可以达到稀疏的效果，是0范式的最优凸近似。L1范式容易求解，是凸的，几乎看的到稀疏约束的地方都是用L1范式。<br><br><strong>贝叶斯先验</strong><br><br>从贝叶斯角度看，加入正则项相当于加入了一种先验，当训练一个模型时，仅依靠当前的训练集是不够的，为了实现更好的汉化能力，往往需要加入先验项。<br></p>
<ul>
<li>L1范数相当于加入了一个拉普拉斯先验</li>
<li>L2范数相当于加入了一个高斯先验<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/WEBRESOURCE83564292b019ae4286754ec067d1b5ba/4492" alt="image"><br><br><strong>过拟合与规则化</strong><br><br>机器学习中出现的非常频繁的问题：过拟合和规则化。常见的有：L0,L1,L2和核范数规则化，然后就是规则化项参数的选择问题。<br><br><strong>监督学习问题无非就是：在规则化参数的同时最小化误差。</strong> 最小化误差是为了让模型拟合训练数据，而规则化参数是防止模型过分拟合训练数据。因此需要保证简单的基础上最小化训练误差，这样的参数才具有好的泛化性能（测试误差小）。模型简单通过规则函数实现的，规则相的使用可以约束模型特性，将人对这个模型的先验知识融入到模型的学习中，强行的让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑。<br><br>奥卡姆剃刀原理：在所有可能或者性能相似的模型中，选择能够很好地解释已知数据并且十分简单的模型。从贝叶斯角度看，规则化项对应于模型的先验概率。民间说法：规则化项是结构风险最小化策略的实现，是在经验风险上加一个正则化项regularizer或惩罚项penalty term。一般来说，监督学习可以看做最小化下面的目标函数：<br><br>$$w^*=argmin\sum_iL(y_i,f(x_i;w))+\lambda\Omega(x)$$<br><br>对于第一项Loss函数，如果是square-loss，就是最小二乘；如果是Hinge Loss(max margin)，就是SVM；如果是exp-Loss，就是boosting；如果是log-loss，就是logistic regression；不同的loss函数具有不同的拟合特性。<br><br>规则化函数$$\Omega(x)$$也有很多选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值越大。规则化项可以是模型参数向量的范数。不同的选择对参数w的约束不同，取得的效果不同。范数聚集在零范数、一范数、二范数、迹范数、核范数。<br></li>
</ul>
<p><strong>L0范数与L1范数</strong><br><br>L0范数指向量中非0元素个数，L0范数规则化一个参数矩阵W，就是希望W大部分元素都是0，让参数W是稀疏的，压缩感知、稀疏编码就是利用L0实现的。L1是求各个元素绝对值和，也叫稀疏规划算子。为什么L1会使权值稀疏，是L0范数的最优凸近似；而且任何的规则化算子，如果在Wi=0的地方不可微，则可以分解为一个求和形式，这个算子就可以实现稀疏。<br><br>L0和L1可以实现稀疏，优点：</p>
<ul>
<li>特征选择feature selection：可以实现特征的自动选择。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，会学习的去掉这些没有信息的特征，也就是把这些特征对应的权重置为0.</li>
<li>可解释性 Interpretability:模型更容易理解。</li>
</ul>
<p><strong>L2范数</strong><br><br>有两个称呼岭回归、权值衰减weight decay。用来解决过拟合。L2范数最小，可以使w每个元素都很小，接近于0.与L1不同，不会让它等于0，而是接近于0。这里有很大区别，越小的参数说明模型越简单，越简单的模型越不容易过拟合。<br><br>L2范数好处：<br></p>
<ul>
<li>学习理论角度：防止过拟合</li>
<li>优化计算角度：L2范数有助于处理condition number不好的情况下矩阵求逆问题，就是奇异矩阵。优化有两大难题:局部最小值；illcondition病态问题。condition number:是一个矩阵的稳定性或者敏感度度量，如果一个矩阵condition number在1附近就是well-conditioned，大于1就是ill-conditioned的。专业点描述：要得到这个解，通常不直接求逆，而是通过解线性方程组方式如高斯消元法来计算，没有规则项时候，矩阵XTX很大，解线性方程组数值上很不稳定，规则项引入可以改善condition number。如果使用迭代优化算法，condition number太大仍然会导致问题，拖慢迭代收敛速度。规则相从优化角度来看，实际上是将目标函数变成$$\lambda-strongly convex$$。<strong>梯度下降中，目标函数收敛速率的上街和矩阵的XTX的condition number有关，越小，上届就越小，收敛速度越快</strong>总结：L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。<br></li>
</ul>
<p><strong>总结L1和L2：L1趋向于产生少量特征，其他特征是0；L2选择更多特征，这些特征都接近于0.Lasso在特征选择非常有用，ridge只是一种规则化。</strong></p>
<p><strong>核范数 nuclear norm</strong><br><br>指矩阵奇异值得和，用于约束Low-rand(低秩)。秩代表了矩阵行列之间的相关性。如果X是一个m行n列的数值矩阵，rank(X)远小于m和n，称X是低秩矩阵。包含大量冗余信息，可以对缺失信息进行恢复，也可以对数据进行特征提取。约束低秩和核范数有什么关系呢，rank()是非凸的，优化问题里很难求解，需要寻找凸近似，核范数$$||W||_*$$就是rank()的凸近似。核范数应用：</p>
<ul>
<li>矩阵填充 matrix completion：应用在推荐系统中，利用低秩矩阵重构。</li>
<li>鲁棒PCA。PCA:找出数据中最主要的元素和结构，去除噪声和冗余，将原有的数据降维，揭示隐藏在复杂数据背后的简单结构。鲁棒性主成分分析Robust PCA:一般我们的数据矩阵包含结构信息也包含噪声，可以将矩阵分解为两个矩阵相加，一个低秩，另一个稀疏的（含有噪声，而噪声是稀疏的）.可以写成$$min rank(A)+\lambda||E||_0,s.t=A+E$$，与经典PCA一样，鲁棒PCA本质上也是寻找数据在低维空间上最佳投影问题。假如X受到随机（稀疏）噪声影响，可能会变成满秩的。PCA假设数据噪声是高斯分布。Robust PCA假设噪声是稀疏的，不管噪声强弱。</li>
<li>背景建模</li>
</ul>
<p><strong>规则化参数选择</strong><br><br>除了loss和规则项两块外，还有个参数$$\lambda$$，叫做超参hyper-parameter。用于平衡loss和规则项，事关模型生死，决定模型性能。$$\lambda$$越大，表示规则项更重要，$$\lambda=0$$，容易过拟合。<br></p>
<hr>
<p>将岭回归和lasso回归一般化，将其看做贝叶斯估计:<br><br>$$\hat{\beta}=argmin{\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|^q}$$<br></p>
<p>$$|\beta_j|^q$$作为$$\beta_j$$的对数先验密度,他们也是参数的先验分布。q=0，相当于子集选择，因为惩罚项统计的是非0参数；q=1，对应的是lasso回归，q=2对应的是岭回归。注意到$$q\leq1$$,前者在方向上不均匀，但在坐标方向集中。<br><br>从这些观点看，lasso、ridge、最佳子集选择都是在不同先验分布情况下的贝叶斯估计。岭回归使用后验均值作为贝叶斯估计，lasso和最佳子集选择不是这样做。<br><br>贝叶斯估计，基于$$\pi(\theta|x1,x2,…,xn)$$对$$\theta$$所作的贝叶斯不急有多种，常用有三种方式：</p>
<ul>
<li>使用后验分布的密度函数最大值作为$$\theta$$的点估计，称为最大后验估计；</li>
<li>使用后验分布的中位数作为$$\theta$$的点估计，称为后验中位数估计</li>
<li>使用后验分布的均值作为$$\theta$$的点估计，称为后验期望估计。</li>
</ul>
<p>Elastic Net是一种使用L1和L2先验作为正则化矩阵的线性回归模型，这种组合用于只有很少权重非零的稀疏模型，比如class:Lasso，但是又能保持Ridge的正则化属性。我们可以使用L1_ration参数来调节L1和L2的凸组合（一类特殊的线性组合）。<br><br>当多个特征和另一个特征相关的时候，弹性网络非常有用，lasso倾向于随机选择其中一个，而弹性网更倾向于选择两个。实践中，Lasso和Ridge之间权衡的一个优势之间权衡的一个优势是它允许循环过程under rotate中继承ridge的稳定性。弹性网的目标函数是最小化：<br><br>$$\lambda\sum_{j=1}^p(\alpha\beta_j^2+(1-\alpha)|\beta_j|)$$</p>
<hr>
<p><strong>最小角回归 least angle regression</strong><br><br>逐步选择forward selection算法(比如forward stepwise regression)在进行子集选择的时候可能会显得太具有侵略性，每次在选择一个变量后都要重新拟合模型，比如第一步选择了一个变量x1，第二步可能会删掉一个和x1相关但也很重要的变量。<br><br>相比forward stagewise是一种比起上面的逐步选择更谨慎的方法，但是可能要经过很多步才能达到最后的模型。就是每次在变量的solution path上前进一小步，而forwar stepwise每次前进一大步。前者带来了高昂的计算代价。<br><br>forward stagewise有着和lasso很大的相似性。<br><br>FS每次根据选择的变量子集，完全拟合出线性模型，计算出RSS，再设计统计量如AIC对较高的模型复杂度做出惩罚，而LARS是每次先找出和因变量相关度最高的那个变量，再沿着LSE的方向一点点调整这个predictor的系数，在这个过程中，这个变量和残差的相关系数会逐渐减少，等到这个相关性没那么显著的时候，就要选进新的相关性最高的变量，然后重新沿着LSE的方向进行变动，而到最后，所有变量都被选中，估计和LSE相同。<br><br>LARS第一次选取的不是最拟合模型的，而是和因变量相关性最强的，算法实际执行步骤如下：<br></p>
<ul>
<li>1、对predictor进行标准化(去除不同尺度的影响)，对target variable进行中心化(去除截距的影响),初始的所有系数都设为0，此时残差r就等于中心化后的target variable</li>
<li>2、找出和残差r相关度最高的变量$$x_j$$</li>
<li>3、将$$x_j$$的系数$$\beta_j$$从0开始沿着LSE(只有一个变量$$x_j$$的最小二乘估计)的方向变化，直到某个新的变量$$x_k$$与残差r的相关性大于$$x_j$$时</li>
<li>4、$$x_j$$和$$x_k$$的系数$$\beta_j,\beta_k$$一起沿着新的LSE(加入新变量$$x_k$$的最小二乘估计)的方向移动，直到有新的变量被选入</li>
<li>5、重复2，3，4直到所有变量被选入，最后得到的估计就是普通线性回归的OLS<br>LARS明显和OLS、Ridge regress等给出了封闭形式的解决方案不同，而是给出了一套对计算机友好的算法。现代统计基本上越来越靠近算法，而和模型无关。<br><br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/WEBRESOURCE7b08ecd764bcce83e5967c80e60cf639/6065" alt="image"><br><br>记$$\hat{\mu}$$为当前拟合值，初始化为0向量，定义$$c(\hat{\mu})$$：<br><br>$$\hat{c}=c(\hat{\mu})=X^T(y-\hat{\mu})$$<br><br>所以$$\hat{c}_j$$正比于变量$$x_j$$和当前残差向量的相关度。<br><br>可以看到，只有两个变量的情况下，当前残差(1)只与y在x1,x2生成的空间上的投影$$\overline{y}_2$$有关，即:<br><br>$$c(\hat{\mu})=X^T(y-\mu)=X^T(y2-\mu)$$<br><br>因为x1与y2-u比x2与y2-u的角度更小，即$$c_1(\hat\mu_0)\gt c2(\hat\mu_0)$$<br><br>LAR算法更新当前值：<br><br>$$\hat\mu_1=\hat\mu_0+\hat y_1x_1$$<br><br>如果是stagewise，那么$$\hat y_1$$是个很小的常数，然后算法重复进行此步骤；如果是逐步选择算法，其是使$$\hat\mu_1$$等于$$\overline{y}_1$$的值；LAR算法采取了二者中一个折中，选取$$\hat y_1$$使得$$\overline y_2-\hat\mu$$与x1,x2相关度相等，也就是说，使得$$y_2-\hat\mu$$评分x1和x2的夹角。<br><br>定义$$\mu_2$$为角平分线方向上的单位向量，则下一步LAR估计是<br><br>$$\hat\mu_2=\hat\mu_1+\hat y_2\mu_2$$<br><br>LARS最后一步，当前活跃集包括了所有自变量，所以步长是任意的，LARS最后一步使得拟合值和OLS的估计值相等。LARS具有较低的计算复杂度，m步的计算开销和OLS的解是同阶的。<br></li>
</ul>
<hr>
<p>LAR和LASSO的自由度<br><br>假定在一定步数之后停止，或者是利用t限制lasso的正则项，那么我们用到多少参数，或者自由度是多少，定义拟合变量$$\hat{y}=(\hat{y_1},\hat{y_2},…,\hat{y_N})$$的自由度:<br><br>$$df(\hat{y})=\tfrac{1}{\sigma^2}\sum_{i=1}^NCov(\hat{y_i},y_i)$$,Cov代表的是预测值和结果的协方差，拟合度越差，协方差越大。<br></p>
<h2 id="对于一个线性模型有k个固定预测属性。很明显-df-hat-y-k-然后岭回归中的定义是-df-hat-y-tr-S-lambda-。在这两种定义的情况下，协方差定义是一种更简单的估计方式，因为-hat-y-H-lambda-y-在y中是线性的-然后对子集选择并没有封闭形式的估计方法。lasso和LAR技术是自适应的，比子集选择更加平滑，自由度更可控。"><a href="#对于一个线性模型有k个固定预测属性。很明显-df-hat-y-k-然后岭回归中的定义是-df-hat-y-tr-S-lambda-。在这两种定义的情况下，协方差定义是一种更简单的估计方式，因为-hat-y-H-lambda-y-在y中是线性的-然后对子集选择并没有封闭形式的估计方法。lasso和LAR技术是自适应的，比子集选择更加平滑，自由度更可控。" class="headerlink" title="对于一个线性模型有k个固定预测属性。很明显$$df(\hat{y})=k$$,然后岭回归中的定义是:$$df(\hat{y})=tr(S_{\lambda})$$。在这两种定义的情况下，协方差定义是一种更简单的估计方式，因为$$\hat{y}=H_{\lambda}y$$在y中是线性的,然后对子集选择并没有封闭形式的估计方法。lasso和LAR技术是自适应的，比子集选择更加平滑，自由度更可控。"></a>对于一个线性模型有k个固定预测属性。很明显$$df(\hat{y})=k$$,然后岭回归中的定义是:$$df(\hat{y})=tr(S_{\lambda})$$。在这两种定义的情况下，协方差定义是一种更简单的估计方式，因为$$\hat{y}=H_{\lambda}y$$在y中是线性的,然后对子集选择并没有封闭形式的估计方法。lasso和LAR技术是自适应的，比子集选择更加平滑，自由度更可控。</h2><p><strong>使用派生输入方向的方法</strong><br><br>有时输入的大量数据之间是相关的，这个方法就是产生一些线性组合。<br><br><strong>1、PCR</strong><br><br>将输入数据进行重新格式化，使输入数据是正交化的，使这个回归是单变量回归的和。输入列$$z_m=Xv_m$$，这个$$v_m$$来自于SVD分解中参数$$X=UDV^T,XV=UD$$然后利用$$z_1,z_2,…,z_m$$回归y值，$$M\leq p$$。<br><br>$$\hat{y}<em>{(M)}^{pcr}=\overline{y}1+\sum</em>{m=1}^M\hat\theta_mz_m$$, $$\hat\theta_m=&lt;z_m,y&gt;/&lt;z_m,z_m&gt;$$<br><br>$$\hat\beta^{pcr}(M)=\sum_{m=1}^M\hat\theta_mv_m$$<br><br>ridge regression和pcr回归很像，岭回归收缩主成分的系数，收缩更多的取决于相应的特征值大小。PCR舍弃p-M个最小的特征值的成分<br><br>主成分分析可以减少自变量个数，还可以用来解决自变量共线性问题。<br><br>方法：借助主成分分析，用主成分回归求回归系数。先用主成分分析计算出主成分表达式和主成分得分变量，而主成分得分变量是相互独立的，因此可以将因变量对主成分得分变量回归，然后将主成分的表达式代回到回归模型中，即可得到标准化自变量与因变量的回归模型，最后将标准化自变量转为原始自变量。<br><br>具体步骤：<br><br>1、用主成分分析法计算出主成分表达式和主成分得分变量，将贡献小的主成分舍去，即求得Z=WX;<br><br>2、用回归分析法将因变量对主成分得分变量进行回归，求得y=AZ;<br><br>3、将主成分的表达式代回到回归模型中，即可得到标准化自变量与因变量的回归模型，即可得到y=AZ=A(WX)=BX;<br><br>4、将标准化自变量转换为原始自变量，即可得到原始自变量与因变量的回归模型。</p>
<p><strong>2、偏最小二乘回归  partial least square</strong><br><br>实际问题中，会遇到需要研究两组多重相关变量间的相互依赖关系，并研究用一组变量(自变量或预测变量)去预测另一组变量（因变量或响应变量），除了最小二乘准则下的经典多元线性回归分析MLR，提取自变量组主成分的主成分回归分析PCR，还有片最小二乘PLS回归方法。<br><br>偏最小二乘回归提供一种多对多线性回归建模的方法，特别当两组变量的个数很多，且都存在多重相关性，而样本量又较少时，用偏最小二乘回归建立的模型具有传统的经典回归分析等方法所没有的优点。偏最小二乘回归分析在建模过程中集中了主成分分析，典型相关分析和线性回归分析方法的特点。<br><br>考虑p个因变量y1,y2,..,yp与m个自变量x1,x2,..,xm的建模问题。偏最小二乘基本做法：在自变量集中提出第一成分u1(是x1~xm的线性组合，尽可能多的提取原自变量中的变异信息)， <strong>同时在因变量集中也提取第一成分v1，并要求u1与v1相关程度最大，然后建立因变量y1~yp与u1的回归，如果回归方程已达到满意的精度，则算法中止。否则对第二成分进行提取，直到能达到满意的精度为止，若最终对自变量集提取r个成分u1,u2,..,ur，偏最小二乘回归将通过建立y1,..,yp与u1,..,ur的回归式，然后再表示为y1,…yp与原自变量的回归方程式，即偏最小二乘回归方程式。</strong> <br><br>算法步骤：<br><br>假定p个因变量$$y_1~y_p$$与m个自变量$$x_1~x_m$$<br><br>1、将自变量组合因变量组进行标准化，并使自变量的均值为0，方差为1;<br>2、分别提取两变量组的第一对成分，并使之相关性达到最大;假设从两组变量分别提出第一对成分u1和v1，u1是自变量集$$X=[x_1,…,x_m]^T$$的线性组合$$u_1=\alpha_{11}x_1+…+\alpha_{am}x_m=\rho^{(1)T}X$$<br><br>v1是因变量$$Y=[y_1,…,y_p]^T$$的线性组合<br><br>$$v_1=\beta_{11}y_1+…+\beta_{1p}y_p=\gamma^{(1)T}Y$$<br><br>为了回归分析需要要求：</p>
<ul>
<li>u1和v1各自尽可能多的提取所在变量组的变异信息；</li>
<li>u1和v1相关程度达到最大<br>对第一成分u1和v1的协方差Cov(u1,v1)可用第一对成分的得分向量的内积来计算，上述的要求可以转化为数学上的条件极值来计算<br><br>3、建立y1…yp对u1的回归及x1,…,xm对u1的回归,再将原变量带入得到新的回归公式<br><br>4、进行交叉有效性检验<br><br>一般情况下，偏最小二乘法并不需要选用存在的r个成分来建立回归式，而像主成分分析一样，选用前l个成分。对于建模所需提取的成分个数l，可以通过交叉有效性检验来确定。<br><br>PLS寻找自变量方差最大和与因变量相关性最高的方向，而PCR只关注方差最大。<br></li>
<li>岭回归收缩所有的方向，更侧重缩减低方差的方向。</li>
<li>PCR只留下M个高方差的方向的列，舍弃剩下的</li>
<li>PLS不仅会收缩低方差方向，而且会导致高方差方向有更高的方差。可能会使这个算法不稳定。</li>
</ul>
<p>如果想要减小预测误差，通常岭回归比子集选择、PAR、PLS更加实用。PCR和PLS只是做了很小的改进。总的来说，PLS、PCR、ridge regression表现的很像。但是岭回归表现的更加平滑，另外两种算法收缩是自离散的步骤下完成。</p>
<hr>
<p><strong>more on the lasso and related path algorithms</strong><br><br><strong>1、增量向前逐段回归</strong><br><br><strong>2、Dantzing selector(DS)</strong><br><br>针对变量数大于样本量选择问题，DS模型：<br><br>$$min_\beta||\beta||<em>1\ subject\ to ||X^T(y-X\beta||</em>\infty\leq t$$<br><br>可以看出来DS算法限制的是相关残差向量$$X^T(Y-X\hat\beta)$$,而不是残差$$Y-X\hat\beta$$.一方面相关残差具有正交变换不变性，而残差没有这种不变性;另一方面相关残差有助于选取与响应变量Y具有高相关性的变量。<br><br>但是这个算法的性能并不尽如人意，算法思想上和lasso是相似的。DS算法试图用所有的预测因子来最小化当前残差的最大内积。<br><strong>3、grouped lasso</strong><br><br>当协变量之间存在一些组结构，进行变量选择时都应该同时选入模型中。比如在分类数据中，通常处理方式是将其变成哑变量，如地域：江苏、浙江、上海。这时需要设置2个哑变量地域=1,浙江;0,非浙江;地狱(2)=1,上海;0,非上海.从而:江苏=0，0；浙江=1，0；上海=0，1;形式如下：<br><br>$$min_{\theta_0\in R,\theta_j\in R^{P_j}}1/2\sum_{i=1}^N(y_i-\theta_0-\sum_{j=1}^Jz_i^T\theta_j)^2+\lambda\sum_{j=1}^J||\theta_j||_2$$<br><br>将会使组合和单个属性进行稀疏性处理。<br><br><strong>4、增量向前逐段回归</strong><br></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/08/数学空间/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/数学空间/" itemprop="url">数学空间</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T09:24:28+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<p>数学中有很多空间，比如欧几里得空间、赋范空间、希尔伯特空间，空间之间有什么关系。<br><br>说空间要从距离说起，距离是一个抽象的概念，定义为：设X为任一非空集，对X任意两点x,y，有一实数d(x,y)与之对应且满足:</p>
<ul>
<li>1、$$d(x,y)\geq 0$$当且仅当x=y;</li>
<li>2、$$d(x,y)=d(y,x)$$</li>
<li>3、$$d(x,y)\leq d(x,z)+d(z,y)$$</li>
</ul>
<p>定义了距离后，再加上线性结构，如向量加法、数乘，使其满足加法交换律、结合律、零元、负元；数乘的交换律、单位一；数乘与加法的结合律共八点要求，形成一个线性空间，这个线性空间就是向量空间。<br></p>
<p>向量空间中，定义了范数的概念，表示某点到空间零点的距离：<br></p>
<ul>
<li>$$||x||\geq 0$$</li>
<li>$$||ax||=|a|||x||$$</li>
<li>$$||x+y||\leq ||x||+||y||$$</li>
</ul>
<p>范数与距离比较，可知距离多了一个条件2，数乘的运算表明了其是一个强化了的距离概念。<br></p>
<p>对范数和距离进行扩展，形成：<br></p>
<ul>
<li>范数集合$$\rightarrow$$赋范空间+线性结构$$\rightarrow$$线性赋范空间<br></li>
<li>距离集合$$\rightarrow$$度量空间+线性结构$$\rightarrow$$线性度量空间<br></li>
</ul>
<p>在已经构成的线性赋范空间上继续扩展，添加内积运算，使空间中有角的概念，形成如下：<br></p>
<ul>
<li>线性赋范空间+内积运算$$\rightarrow$$内积空间<br></li>
</ul>
<p>这时内积空间已经有了距离、长度、角，有限维的内积空间就是我们熟悉的欧式空间。<br></p>
<p>继续在内积空间上扩展，使得内积空间满足完备性，形成希尔伯特空间：<br></p>
<ul>
<li>内积空间+完备性$$\rightarrow$$希尔伯特空间<br></li>
</ul>
<p>完备性的意思就是空间中的极限运算不能跑出该空间，如有理数空间中的$$\sqrt 2$$的小数表示，其极限随着小数位数增加收敛到$$\sqrt 2$$,但其属于无理数，并不在有理数，不满足完备性。<br></p>
<p>距离 $$\rightarrow$$范数 $$\rightarrow$$内积 <br></p>
<p>(向量空间+范数)$$\rightarrow$$(赋范空间+线性结构)$$\rightarrow$$(线性赋范空间+内积运算)$$\rightarrow$$(内积空间+完备性)$$\rightarrow$$(希尔伯特空间)<br></p>
<p>(内积空间+有限维)$$\rightarrow$$(欧几里得空间)<br></p>
<p>(赋范空间+完备性)$$\rightarrow$$(巴拿赫空间)<br></p>
<p>对距离弱化，保留距离的极限和连续概念，就形成了拓扑概念。</p>
<p>定义一个函数空间中无穷维的矩阵K(x,y)<br><br>其满足：<br><br>1、正定性，即对于任意函数f(x)，都有<br><br>$$\int\int f(x)K(x,y)f(y)dxdy\geq 0$$<br></p>
<p>2、对称性即K(x,y)=K(y,x)<br><br>$$\int K(x,y)\phi(x)dx=\lambda\phi(y)$$<br><br>则这个二元函数可称为核函数，可证明一个核函数的特征函数之间是正交的。<br><br>$$&lt;\phi_1,\phi_2&gt;=\int\phi_1(x)\phi_2(x)dx=0$$<br><br>所以$${\phi_i}^\infty_{i=1}$$是原来函数空间的一组基底。<br></p>
<p>同时原来的核函数也可以分解成为核函数之间相乘的结果：<br></p>
<h2 id="K-x-y-sum-i-0-infty-lambda-i-phi-i-x-phi-i-y"><a href="#K-x-y-sum-i-0-infty-lambda-i-phi-i-x-phi-i-y" class="headerlink" title="$$K(x,y)=\sum_{i=0}^\infty\lambda_i\phi_i(x)\phi_i(y)$$"></a>$$K(x,y)=\sum_{i=0}^\infty\lambda_i\phi_i(x)\phi_i(y)$$<br></h2><p><strong>再生核希尔伯特空间RKHS</strong><br><br>RKHS是由核函数构成的空间。基底为$${\sqrt\lambda_i\phi_i}<em>{i=1}^\infty$$<br><br>任意函数都可以由基底表示:<br><br>$$f=\sum</em>{i=1}^{\infty}f_i\sqrt\lambda_i\phi_i$$<br><br>选取其中两个函数，按照坐标表示如下：<br><br>$$f=(f_1,f_2,..)^T_h$$和$$g=(g_1,g_2,…)^T_h$$<br><br>其内积为:$$&lt;f,g&gt;<em>h=\sum</em>{i=1}^{\infty}f_ig_i$$<br><br>再来看核函数K(x,y)将其中一个元素固定K(x0,y),则其也可以看成一个函数<br><br>$$K(x_0,y)=\sum_{i=0}^{\infty}\lambda_i\phi_i(x)\phi_i$$<br></p>
<p>化成向量坐标表示形式：<br><br>$$K(x_0,y)=(\sqrt\lambda_1\phi_1(x),\sqrt\lambda_2\phi_2(x),…)_h^T$$<br><br>所以计算内积：<br><br>$$&lt;K(x_0,y),K(y_0,x)&gt;<em>h=\sum</em>{i=0}^{\infty}\lambda_i\phi_i(x_0)\phi_i(y_0)=K(x_0,y_0)$$<br><br>这个性质就叫做再生性reproducing property，所以这个空间才叫再生希尔伯特空间。这个性质很好，原本函数之间计算内积需要算无穷维积分的，但是现在只需要计算核函数就好了。<br><br>所以给定一个点的映射一个点x给其映射到一个无穷维的特征空间【即先将这个点变成一个函数，而这个函数可以看成一个无穷维的特征空间】：<br><br>$$\phi(x)=K(x,\cdot)=(\sqrt\lambda_1\phi_1(x),\sqrt\lambda_2\phi_2(x),…)^T$$<br></p>
<p>得到两个特征空间上内积为：<br><br>$$&lt;\phi(x),\phi(y)&gt;_h=&lt;K(x,\cdot),K(y,\cdot)&gt;_h=K(x,y)$$<br>这就是kernel trick</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/07/假设检验/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/07/假设检验/" itemprop="url">假设检验</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-07T23:44:44+08:00">
                2018-07-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><br><strong>零假设Null hypothesis 备择假设 alternative hypothesis</strong><br><br>参数假设检验中，假设是对总体参数的具体数值所作的陈述。为了使得作为证据的样本统计量必然支持且仅支持一个假设，要建立对于总体参数在逻辑上完备互斥的一对假设，即原假设null pypothesis,记为$$H_0$$,备择假设alternative hypothesis,记为$$H_1$$<br><br>原假设（零假设）假定总体参数未发生变化，备择假设，假定总体参数发生变化。实际建立假设时，原假设与备择假设方向不同，会导致不同的结论。在选择原假设和备择假设时，通常根据研究者是希望收集证据予以支持还是拒绝的判断作为选择依据。<br><br><strong>实际操作中，通常研究者希望收集证据予以拒绝的假设作为原假设，而将研究者希望通过搜集证据予以支持的假设作为备择假设。</strong><br><br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/64F9BD4B4A324D32A8F2A6709AA2240D/2963" alt="image"></p>
<hr>
<p><strong>参数估计的方法</strong><br><br>主要分为点估计和区间估计。点估计的方法有矩估计法、顺序统计量、极大似然法、最小二乘法。<br><br>点估计：1：用样本的估计量直接作为总体参数的估计值。例如用样本均值直接作为总体均值的估计。2:没有给出估计值接近总体参数程度的信息，没有给出可信程度，矩估计、极大似然估计。<br><br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/9F7BCBC2331342548ED8EE7CA4CA3119/3201" alt="image"><br><strong>评价估计量的标准</strong><br><br>1、无偏性 unbiasedness<br><br>估计量抽样分布的数学期望等于被估计的总体参数。$$E(\hat{\theta})=\theta$$<br><br>2、有效性 efficiency<br><br>对同一总体参数的两个无偏点估计量，有更小标准差的估计量更有效。$$D(\hat{\theta_1})&lt;D(\hat{\theta_2})$$<br><br>3、一致性 consistency<br><br>随着样本容量的增大，估计量的值越来越接近被估计的总体参数$$\lim_{n\rightarrow{\infty}}P(|\hat{\theta}-\theta|&lt;\epsilon)=1$$</p>
<hr>
<p><strong>假设检验Hypothesis Test</strong><br><strong>统计推断包括：参数估计和假设检验</strong><br><br><strong>1、假设检验</strong><br><br>亦称显著性检验，是对总体的参数或分布作出某种假设，然后利用适当的方法，根据样本对总体提供的信息，对此假设作出推断。（拒绝或不拒绝）<br><br><strong>基本原理</strong><br><br>1、逻辑学上的反证法，先建立假设，然后提供假设成立与否的证据；<br></p>
<h2 id="2、小概率事件原理。在一次机会中几乎不发生的原理。"><a href="#2、小概率事件原理。在一次机会中几乎不发生的原理。" class="headerlink" title="2、小概率事件原理。在一次机会中几乎不发生的原理。"></a>2、小概率事件原理。在一次机会中几乎不发生的原理。</h2><p><strong>卡方分布 t分布  F分布 t检验 f检验</strong><br><br><strong>1、卡方</strong><br><br><strong>A、卡方分布</strong><br><br>若N个相互独立的随机变量$$\xi_1,\xi_2,…,\xi_n$$均服从标准正态分布，也称独立同分布与标准正态分布，则这n个服从标准正态分布的随机变量平方和$$Q=\sum_{i=1}^n\xi^2_i$$构成新的随机变量，其分布律称为$$\chi^2$$分布chi-square distribution，其中参数n称为自由度，比如正太分布中均值或fangcha1不同就是另一个正态分布一样，自由度不同就是另一个分布,记为$$Q\sim\chi^2(k)$$。当自由度n很大时，$$\chi^2$$分布近似为正态分布。对于任意正整数k，自由度为k的卡方分布是一个随机变量x的机率分布。<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/8CF385741FD04FBBA7D90279C4212076/2821" alt="image"><br>机率函数为：<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/C70631BCF15444C897058CECFC0B9418/2824" alt="image"><br><br>$$E(\chi^2)=n,D(\chi^2)=2n$$<br><br><strong>B、卡方检验</strong><br><br>卡方分布用途：检查实际结果与期望结果之间何时存在差异。<br><br>1、检验拟合优度：可以检验一组给定数据与指定分布的吻合程度。如：用它检验抽奖机收益的观察频数与我们所期望的吻合程度。<br><br>2、检验两个变量的独立性：通过这个方法检查变量之间是否存在某种关系。如：独居结果是否取决于坐庄的庄家，庄家是否暗箱操作。<br><br><strong>显著性</strong><br><br>卡方分布指出观察频数与期望频数之间差异显著性，和其他假设一样，取决于显著性水平。1、显著性水平$$\alpha$$进行检验，则写作：（常用的显著水平有1%和5%）$$\chi^2_{\alpha}(v)$$<br><br>检测标准：<strong>卡方分布检验是单尾检验且是右尾，右尾被作为拒绝域。通过查看检验统计量是否位于右尾的拒绝域以内，来判定期望分布得出结果可能性。</strong>卡方临界值表可以查询的。<strong>比如：5%的显著性水平，8的自由度进行检验。查出15.51，只要检验统计量大于15.51，检验统计量就位于拒绝域内。</strong><br><br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/5713FCB95B2241418C268B71E4C4379A/2862" alt="image"><br><br><strong>检验统一量$$x^2=\sum\frac{(O-E)^2}{E}$$</strong><br><br>解决第一类问题：<br><br>原假设$$H_0$$：老虎机每局收益符合下述概率分布；备择假设$$H_1$$:老虎机每局收益不符合概率分布 <strong>[期望证明的]</strong><br><br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/6290E44701A4443F8E41771C59426AEA/2901" alt="image"><br><br>所以$$x^2=(965-977)^2/977+…+(7-1)^2/1$$,然后再利用$$x^2$$和显著性水平$$\alpha$$进行比较。<br><br>解决第二类问题：<br><br>原假设$$H_0$$：赌局输赢结果和坐庄的庄家没有关系；备择假设$$H_1$$:赌局输赢和坐庄的庄家有关系。<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/FDC530BC6E1D43769D76A5E7BB4832F9/2916" alt="image"><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/5791599DA2324D3F865FE05433523E84/2918" alt="image"><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/5FA8B1BA1F5E41CD98A98B3CCE9078E8/2917" alt="image"></p>
<p><strong>本质上，自由度是做一个估计时，所拥有的独立信息的数量</strong></p>
<p><strong>1、拟合优度检验中，v=组数-限制数</strong><br></p>
<p><strong>2、两个变量独立性检验中,如列联表为h行k列，则：v=(h-1)x(k-1)</strong></p>
<hr>
<p><strong>t分布 t假设检验</strong><br><br><strong>A、t分布</strong><br><br>根据中心极限定理，在正态分布总体以固定n，抽取若干样本时，样本均数$$\overline{X}$$的分布仍服从正态分布，即$$N(\mu,\frac{\sigma^2}{n})$$,对样本均数的分布进行u变换，也可变换为标准正态分布N(0,1)。<br>实际中，往往$$\sigma$$(总体方差)是未知的，常用s（样本方差）作为$$\sigma$$的估计值，为了与u变换区别，称为t变换，统计量t值得分布称为t分布。假设X服从标准正态分布N(0,1)，Y服从$$\chi^2(n)$$分布，那么$$Z=\frac{X}{\sqrt{Y/n}}$$的分布称为自由度为n的t分布，记为$$Z\sim t(n)$$，但实际自由度为v=n-1，查表的时候就是按照这个来查。<br><br>特点：t分布以y轴，左右对称。只有一个参数v=n-1,曲线形状与自由度有关。自由度v趋向于$$\infty$$，t分布逼近u分布，标准正态分布是t分布的特例。t分布曲线下的面积是1.<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/555684CA43E04E9F8A398F739EC86904/3011" alt="image"><br><br><strong>B、t假设检验</strong><br><br><strong>总体均数估计</strong><br><br><strong>A、点（值）估计point estimation</strong><br><br>用样本统计量直接作为总体参数的估计值。<strong>矩估计法、顺序统计量法、极大似然、最小二乘</strong>举个例子：为了了解某地1岁婴儿体重，随机抽取1岁婴儿25个，平均体重2kg，标准差0.2kg。试估计本地1岁婴儿平均体重<br><br><strong>B、区间估计</strong><br><br>按预先给定的概率，计算一个区间，使它能够包含未知的总体均数。事先给定的概率$$1-\alpha$$称为可信度，通常取$$1-\alpha=0.95$$<br><br>可信度（置信率，置信度）：由样本推断总体特征时，估计正确的概率，用$$1-\alpha$$表示。可信区间confidence interval，按预先给定的概率确定的包含未知总体参数的可能范围。构成可信区间的两个点值称为：下限值、上限值。可信度计算分为两种$$\sigma$$</p>
<p><br>(1)已知，$$\mu=\frac{\overline{X}-\mu}{\sigma-\sqrt{n}}$$,知道$$P(-1.96&lt;=u&lt;=1.96)=0.95$$,则95%的置信区间$$(\overline{X}-1.96\sigma_{\overline{X}},\overline{X}+1.96\sigma_{\overline{X}})$$,更一般的形式：$$(\overline{X}-u_{\alpha/2}\sigma_{\overline{X}},\overline{X}+u_{\alpha/2}\sigma_{\overline{X}})$$<br></p>
<p>(2)$$\sigma$$通常未知，这时可以用其估计量S代替，但$$(\overline{X}-\mu)/(S/\sqrt{n})$$已不再服从标准正态分布，而是服从著名的t分布。计算可信区间的原理与前面完全相同，仅仅是两侧概率界值有些差别，即：<br><br>$$P(-t_{\alpha/2(v)}&lt;u&lt;t_{\alpha/2(v)})=1-\alpha$$<br><br>可信区间：$$(\overline{X}-t_{\alpha/2(v)}<em>S_{\overline{X}},\overline{X}+t_{\alpha/2(v)}</em>S_{\overline{X}})$$<br><br>可以得到总体均值的区间估计</p>
<hr>
<p><strong>F分布及假设检验</strong><br><br><strong>A、F分布</strong><br><br>Z检验是一般用于大样本(样本容量大于30)平均值差异性检验的方法，用标准正太分布的理论来推断差异发生的概率，从而比较两个平均数的差异是否显著，又被称为u检验。为什么要引入F分布。比如研究A、B、C三种不同学校学生的阅读理解成绩，以为简单多次使用Z检验或t检验，成对比较即可。但是这两种检验也有其局限性，比如研究中出现两个以上的平均数时。<strong>1、比较的组合次数增多。</strong> <strong>2、降低可靠程度，对数据做的Z检验或t检验越多，更容易范I型错误。</strong> 比如一个检验中$$\alpha=0.05$$，即有0.95的概率不犯I型错误，那么两次0.95*0.95=0.9,此时犯I型错误的概率是0.1，翻倍。 <strong>3、缺少综合或整体信息</strong>两个以上的平均数检验中若仍采用Z检验或t检验都只提供了两个组所提供的的信息，而忽略了其余的综合信息。许多情况下这些被忽略的信息可能对检验结果产生更大的影响。同时在十次检验之后所得到的只是零散的信息，并非从总体来分析几种不同条件的效果，也难以获得几种不同条件的直接答案。<br><br><strong>方差分析：</strong> analysis of variance就是对多个平均数进行比较的一种统计方法，又称为变异数分析，即ANOVA。方差分析的三条假设，否则易产生错误的统计结论：A总体分布的正态性；样本必须来自正态分布的总体；B各个实验组方差齐性； C变异具有可加性。影响事物的因素多种多样，方差分析将事物的总变异分解为各个不同变异来源，分解后各部分变异相互独立，相加后构成总变异。<br><br>方差分析有广义与侠义之分。广义方差分析包括方差齐性检验、F检验和多重比计较（逐对平均数的比较）。侠义的方差分析仅指F检验。<br><br><strong>F分布</strong><br><br>若总体$$X\sim N(0,1)$$,(X1,X2,..,Xn),(Y1,Y2,…,Yn)为来自X的两个独立样本，设统计量$$F=\frac{\chi(X)/n1}{\chi{(Y)}/n2}$$，称F服从自由度n1和n2的F分布。记为$$F\sim F(n1,n2)$$<br><br>若总体$$X\sim N(\mu,1)$$,与总体$$Y\sim N(0,1)$$，(X1,X2,..,Xn)为来自X的样本,(Y1,Y2,…,Yn)为来自Y的一个样本，则统计量$$F=\frac{\chi(X)/n1}{\chi{(Y)}/n2}$$称F服从自由度为n1和n2，非中心参数$$\delta=n\mu^2$$的非中心F分布，记为$$F\sim F(n1,n2,\delta)$$<br><br><strong>F分布–方差同质性检验</strong><br><br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/0CF3B8D93AA849058904AF8FFFDEC43E/3312" alt="image"><br><br>组与组之间的差异称为组间变异variation between classes，反映在各组的平均数不同。同一组内部被试个体之间的差异称为组内变异variation within class，反映在每一个的分数不同。<br>10-1组间变异较大组内变异较小，10-2反之。<br><br><strong>原理：通过组间变异和组内变异比率大小来推论几个相应平均数差异显著性的思想就是方差分析的逻辑依据或基本原理。</strong>方差分析是将实验中总变异分解为组间变异和组内变异，并通过组间变异和组内变异比率的比较来确定影响实验结果因素的数学方法，实质是以方差来表示变异的程度。<br><br><strong>总变异：组间变异、组内变异。</strong> 组间变异：实验条件、随机误差。组内变异：个体差异、实验误差，都属于随机误差。如果组间与组内变异均为随机误差时，二者比率为1，即实验因素的影响较小，由此推论总变异不存在差异。二者比率较大时，实验因素产生影响可能性增大。<br>分析基本过程：<br><br>(一)变异内容与表达<br><br>总变异=组间变异+组内变异<br><br>变异(Variance)即方差$$S^2$$，又称为均方差或均方(Mean Square,MS).$$S^2=\tfrac{\sum(X-\overline{X})^2}{n-1}$$，分母为自由度，总变异及各变异原因记为：$$MS_t=MS_b+MS_w$$<br><br>方差分析是一种参数检验方法，进行均数差异的检验时必须考虑作为参数检验应该具备的条件，应该考虑不同总体的变异水平–个体差异是否一致。<br><br>(二)方差齐性检验test of homogeneity of variance<br><br>方差齐性检验的虚无假设是假设各个总体方差相等，即无显著差异或是各个样本方差来自相同总体，表达式为:$$H_0=\sigma_1^2=\sigma_2^2…==\sigma_k^2$$检验多个总体方差一致性的方法，这是最常用的哈特莱检验法。<br><br>研究假设虽然不能保证所有的方差存在显著差异，但可以假设至少有两总体方差存在显著差异，只要有两种总体方差或样本方差不一致，虚无假设各总体方差相等就不成立了。<br><br>借助于F最大值来检验，F最大值就是把一系列方差中最大方差与最小方差进行比较的方法。即：<br>$$F_{max}=\tfrac{S^2_{n-1}max}{S^2_{(n-1)min}}$$,·在虚无假设下F最大值的分布的临界值已由哈特莱计算出来形成了F最大值理论分布表。查F值时，需根据方差数目及方差的自由度进行。其中自由度为n-1.比如对左图的数据进行方差齐次检验方法如下：<br></p>
<ol>
<li>建立假设$$H_0:\sigma_1^2=\sigma_2^2=\sigma_3^2$$, $$H_1$$:至少有两个总体的方差存在显著差异。求各样本的方差：$$S^2=\tfrac{\sum X^2-(\sum X)^2/n}{n-1}$$ <br><br>$$S_A^2=\tfrac{160-30^2/6}{6-1}=2$$ $$S_B^2=0.8$$  $$S_C^2=2.8$$<br><img src="https://note.youdao.com/yws/public/resource/5702a21be94cca30edfd03987e6050c7/xmlnote/WEBRESOURCEba15eb74ea2de6b777e6464298dbf1bb/3419" alt="image"></li>
<li>求F最大值<br><br>$$F_{max}=2.8/0.8=3.5$$<br><br>3.比较与决策<br><br>当组数k=3，自由度为6-1=5，$$F_{max}=10.8$$因为F=3.5&lt;$$F_{max0.05}$$=10.8,P&gt;0.05，差异不显著，接受虚无假设，拒绝研究假设，说明三个总体方差一致。<br></li>
</ol>
<p><strong>F分布–方差分析</strong><br><br>样本容量相等的方差分析，意味着每一种实验处理它们的被重复次数相同，每一种学习方法均重复了6次。方差分析过程与上一节所介绍的方差分析基本方法完全一致。<br><br>样本容量不相同的方差分析，完全随机的方差分析中，常常使各实验处理组的被试数目相等。这本不需要，但能使计算稍微容易些，像独立样本t检验一样，F检验也允许样本容量不等。<br><br><strong>方差分析是比较两个以上平均差异显著性的方法逻辑思想：将总变异分解成组间变异或组内变异，通过比较组间与组内变异率的大小来确定均数差异是来自实验因素或处理，还是源自随机误差。</strong><br><br>引起组间的变异原因主要是实验施加的影响因素和随机误差，组内变异的原因是随机误差（个体变异和实验变异）。根据一次实验因素的个数分为单因素实验和多因素实验，单因素实验方差分析主要有完全随机设计的方差分析和完全随机区组设计的方差分析。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hcyxy.tech/2018/07/07/overview-of-supervised-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hecy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="万水千山">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/07/overview-of-supervised-learning/" itemprop="url">overview of supervised learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-07T22:24:48+08:00">
                2018-07-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script></p>
<h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><p>bivariate gaussian二元高斯分布<br></p>
<h3 id="linear-regresion-VS-KNN"><a href="#linear-regresion-VS-KNN" class="headerlink" title="linear regresion VS KNN"></a>linear regresion VS KNN</h3><p>1、linear regression 具有低偏差 高方差的特点<br><br>2、KNN具有高偏差 低方差的特点<br><br>KNN所产生的误差随k的增加而增加,在高维度下面会出现维度爆炸问题，本质上就是数据稀疏化，如果遇到特殊的分布，函数值还可能只集中在边缘部分。</p>
<h3 id="模型选择准则"><a href="#模型选择准则" class="headerlink" title="模型选择准则"></a>模型选择准则</h3><p><strong>1、EPE(Expected Predictor Error)</strong> 期望预测误差<br><br>$$EPE=E(Y-E(X))^2=\int[y-f(x)]^2Pr(dx,dy)$$</p>
<p>进一步化简：<br><br>$$EPE(f)=ExE_{Y|X}([Y-f(X)]^2|X)$$<br><br>$$f(x)=argmin_cE_{Y|X}([Y-c]^2|X=x)$$<br></p>
<h2 id="当采用均方差时，最好的预测是条件均值：f-x-E-Y-X-x"><a href="#当采用均方差时，最好的预测是条件均值：f-x-E-Y-X-x" class="headerlink" title="当采用均方差时，最好的预测是条件均值：f(x)=E(Y|X=x)"></a>当采用均方差时，最好的预测是条件均值：f(x)=E(Y|X=x)</h2><p><strong>2、EPE(x0)</strong> 的计算 <br><br>令$$u=E_{y_0|x_0}(y_0)=x_0^Tβ$$<br><br>$$EPE(x_0)=E_{y_0|x_0}E_D(y_0-y)^2=(y_0^2-u^2)+(E_D(y^2)-(E_D(y))^2)+((E_D(y))^2-2y_0E_Dy+u^2)$$<br></p>
<h2 id="即：-E-y-0-x-0-E-D-y-0-y-2-Var-y-0-x-0-Var-y-Bias-2-y"><a href="#即：-E-y-0-x-0-E-D-y-0-y-2-Var-y-0-x-0-Var-y-Bias-2-y" class="headerlink" title="即：$$E_{y_0|x_0}E_D((y_0-y)^2)=Var(y_0|x_0)+Var(y)+Bias^2(y)$$"></a>即：$$E_{y_0|x_0}E_D((y_0-y)^2)=Var(y_0|x_0)+Var(y)+Bias^2(y)$$<br></h2><p>3、如果一个算法在等方向的相邻区域产生局部变化函数无法克服高维爆炸问题；克服了等方向问题，又会出现在各个方向不等变化。<br></p>
<h2 id="EPE得到的是一个随机变量-XX-T-线性模型得到的是一个数据矩阵：-X-TX"><a href="#EPE得到的是一个随机变量-XX-T-线性模型得到的是一个数据矩阵：-X-TX" class="headerlink" title="EPE得到的是一个随机变量$$XX^T$$ 线性模型得到的是一个数据矩阵：$$X^TX$$"></a>EPE得到的是一个随机变量$$XX^T$$ 线性模型得到的是一个数据矩阵：$$X^TX$$<br></h2><p><strong>4、模型的决定</strong><br><br>模型需要确定一个平滑的或者是复杂的参数：</p>
<ol>
<li>惩罚项的乘数</li>
<li>核的宽度</li>
<li>或基函数的数量</li>
</ol>
<hr>
<p>5、期望预测误差<br></p>
<h2 id="EPE-k-X-0-E-Y-f-k-x-0-2-X-x-0-θ-2-Bias-2-f-k-x-0-Var-D-f-k-x-0-θ-2-f-x-0-frac-1-k-sum-l-1-kf-x-k-2-θ-2-k"><a href="#EPE-k-X-0-E-Y-f-k-x-0-2-X-x-0-θ-2-Bias-2-f-k-x-0-Var-D-f-k-x-0-θ-2-f-x-0-frac-1-k-sum-l-1-kf-x-k-2-θ-2-k" class="headerlink" title="$$EPE_k(X_0)=E[(Y-f_k(x_0))^2|X=x_0]=θ^2+[Bias^2(f_k(x_0))+Var_D(f_k(x_0))]=θ^2+[f(x_0)-\frac{1}{k}\sum_{l=1}^kf(x_k)]^2+θ^2/k$$"></a>$$EPE_k(X_0)=E[(Y-f_k(x_0))^2|X=x_0]=θ^2+[Bias^2(f_k(x_0))+Var_D(f_k(x_0))]=θ^2+[f(x_0)-\frac{1}{k}\sum_{l=1}^kf(x_k)]^2+θ^2/k$$<br></h2><p>利用加权最小二乘解决异方差问题，异方差问题就是随着变量变化，方差值会不同，比如高收入家庭支出占比变化大；</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">hecy</p>
              <p class="site-description motion-element" itemprop="description">dreamer</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">71</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hecyxy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:hcy_xy@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-gmail"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hecy</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
